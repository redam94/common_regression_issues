"""Data simulated with group effects, data is now correlated."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/synthetic_data/01_grouped_data.ipynb.

# %% auto 0
__all__ = ['generate_grouped_data']

# %% ../../nbs/synthetic_data/01_grouped_data.ipynb 5
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import xarray as xr
from typing import Optional

# %% ../../nbs/synthetic_data/01_grouped_data.ipynb 6
def generate_grouped_data(
    sample_size: int, # Number of samples per group
    n_exogenous_vars: int, # Number of exogenous variables
    n_confounders: int = 0, # Number of confounder variables
    n_groups: int = 1, # Number of independent groups
    n_group_attributes: int = 0, # Number of group level attributes
    group_var: float = 3, # Variance between groups
    noise_sigma: float = 1, # Std of noise for each observation
    random_effect_assumption_satified: bool = True, # Is random effect assumption valid
    random_seed: Optional[int] = None
) -> xr.Dataset: # Synthetic dataset
    "Generate grouped regression data"
    # Input checks
    assert sample_size > 0, "Sample size must be positive"
    assert n_exogenous_vars > 0, "Number of exogenous variables must be positive"
    assert n_confounders >= 0, "Number of confounders must be positive or zero"
    assert n_groups > 0, "Number of groups must be positive"
    rng = np.random.default_rng(random_seed)
    group_names = [f"group_{i}" for i in range(n_groups)]
    exog_var_names = [f"var_{i}" for i in range(n_exogenous_vars)]
    confounder_names = []
    confounder_data = np.zeros(shape=(1, n_groups, sample_size))
    confounder_beta = np.zeros(shape=(n_exogenous_vars, 1))
    if n_confounders>0:
        confounder_names = [f"con_{i}" for i in range(n_confounders)]
        confounder_data = rng.normal(0, 1, size=(n_confounders, n_groups, sample_size))
        confounder_beta = rng.normal(0, 1, size=(n_exogenous_vars, n_confounders))
    
    group_var_offset = rng.normal(0, group_var, n_groups)
    group_confound = np.zeros_like(group_var_offset)

    if not random_effect_assumption_satified:
        group_confound = rng.normal(group_var_offset, .2, n_groups)

    exog_data = rng.normal(
        0, 1, 
        size=(n_exogenous_vars, n_groups, sample_size)
        )
    exog_data = exog_data + np.einsum('ij,jkl->ikl', confounder_beta, confounder_data)
    exog_data = exog_data + group_confound[None, :, None]
    
    exog_betas = rng.normal(0, 1, size=n_exogenous_vars)
    if n_confounders>0:
        confounder_beta = rng.normal(0, 1, size=n_confounders)
    
    dep_var = (
        np.einsum('i,ikl->kl', confounder_beta, confounder_data) 
        + np.einsum('i,ikl->kl', exog_betas, exog_data)
        + group_var_offset[..., None] + group_confound[..., None]
        + noise_sigma * rng.normal(0, 1, size=(n_groups, sample_size))
    )
    _index = np.arange(sample_size)
    variables = {}
    for i, var in enumerate(exog_var_names):
        variables[var] = xr.DataArray(
            exog_data[i], 
            coords={'group': group_names, 'index': _index},
            dims=('group', 'index')
        )
    for i, var in enumerate(confounder_names):
        variables[var] = xr.DataArray(
            confounder_data[i],
            coords={'group': group_names, 'index': _index},
            dims=('group', 'index')
        )
    variables['depvar'] = xr.DataArray(
        dep_var,
        coords={'group': group_names, 'index': _index},
        dims=('group', 'index')
    )
    dataset = xr.Dataset(variables)
    dataset = dataset.assign_attrs(
        true_alpha=group_var_offset[..., None] + group_confound[..., None],
        true_betas={var_name: np.round(exog_betas[i], 4) for i, var_name in enumerate(exog_var_names)})
    return dataset    
