{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: \n",
    "  - name: Matthew Reda\n",
    "    email: redam94@gmail.com\n",
    "license: \"CC BY\"\n",
    "copyright: \n",
    "  holder: Matthew Reda\n",
    "  year: 2024\n",
    "citation: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Illusion of Significance\n",
    "\n",
    "> The Cost of p-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "abstract: |\n",
    "    Statistical models drive millions in spending decisions, yet beneath their precise-looking numbers lurks a dangerous problem. This post examines how the practice of selecting variables based on p-values creates a statistical house of cards; especially in Marketing Mix Modeling (MMM) and budget optimization. I show why common techniques like stepwise regression inevitably produce overconfident models with biased estimates that violate the very statistical principles they claim to uphold. These methodological flaws can result in money flowing to the wrong marketing channels based on **illusory** performance metrics. I demonstrate why Bayesian approaches offer a more honest alternative by naturally tempering overconfidence, incorporating what we already know, and providing intuitive uncertainty measures. Through techniques like spike-and-slab priors (or regularized horseshoe priors) and Bayesian Model Averaging (BMA), analysts can move beyond arbitrary significance thresholds toward probability-based decision-making. While Bayesian methods do require more computational horsepower and thoughtful prior specification, modern software has made them increasingly accessible. Using simulated examples inspired by real-world marketing and economic modeling, I show how Bayesian methods produce more reliable insights that lead to smarter budget allocation decisions.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Illusion to Insight — TLDR\n",
    "\n",
    "We've all seen it (perhaps even presented it); the perfectly constructed model with impressive-looking p-values that is used to justify adding millions in budget to a particular marketing channel. But how much can we really trust these insights? Statistical modeling approaches that rely on p-value thresholds (or $\\left| \\text{t-stat}\\right| > 1$) for variable selection create a fundamental problem: the same data that builds the model also validates it, creating a circular logic that **inevitably** results in overconfidence. Methods like stepwise regression don't just produce slightly inaccurate models: they systematically generate **biased coefficients**, **invalid statistical tests**, and **unstable predictions**. When these flawed models drive budget decisions, organizations end up misallocating resources based on what amounts to **statistical mirages** rather than **genuine insights**.\n",
    "\n",
    "Bayesian statistics offers a more realistic alternative that acknowledges what traditional methods try to hide. **Uncertainty is real!** It should inform our decisions and not be artificially removed through statistical hocus-pocus for a slick deck. By treating parameters as variables with probability distributions and formally incorporating prior knowledge, Bayesian methods can help regularize models; prevent them from making wild claims the data can't support. Techniques like spike-and-slab priors let us evaluate which variables truly matter without arbitrary cutoffs, while Bayesian Model Averaging sensibly hedges bets across multiple plausible models rather than on a single one. Though implementing Bayesian approaches does require more computational power, technical skill, and careful thought about prior assumptions, the payoff is substantial: **more stable estimates**, **intuitive uncertainty measures** that stakeholders can actually understand (seriously what is a confidence interval), and ultimately, budget allocation **insights** based on reality rather than **statistical illusions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=true}\n",
    "## What is a p-value?\n",
    "From \"The ASA Statement on p-Values: Context, Process, and Purpose\":\n",
    "\n",
    "> Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value\n",
    "\n",
    "In the context of linear regression the test statistic for the i-th coefficient estimate is given as: \n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta_i}}{\\frac{\\hat{\\sigma}}{\\sqrt{n\\hat{\\text{Var}}[X_i]}}\\sqrt{VIF_i}}\n",
    "$$\n",
    "\n",
    "It's clear from the mathematical description of the test statistic that:\n",
    "\n",
    "1. Coefficients ($\\hat{\\beta_i}$) with larger magnitudes — all else being equal — will be more \"significant\". \n",
    "2. Models with lower variance ($\\hat{\\sigma}$) will lead to all coefficients being more \"significant\".\n",
    "3. Larger sample sizes (n) will increase all coefficients t-stat.\n",
    "4. With increased variance in a predictor ($\\hat{\\sigma}$) — all else being equal — that predictor will be more \"significant\".\n",
    "5. With increased correlation of covariate $X_i$ ($VIF_i$) — all else being equal — that predictor will be less \"significant\".\n",
    "   \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Modeling for Decision Making\n",
    "\n",
    "### The Hidden Costs of Misspecification: Why Model Selection Matters\n",
    "\n",
    "Statistical models, particularly within econometrics and marketing analytics, serve as critical instruments for dissecting complex business phenomena and guiding strategic decisions. Marketing Mix Modeling (MMM) stands as a prime example. The insights derived from MMM are frequently used for two primary purposes: understanding the historical contribution of different marketing levers and, crucially, optimizing future budget allocations to maximize return on investment (ROI). \n",
    "\n",
    "MMM's use extends beyond prediction toward causal inference: understanding the \"true\", incremental impact of altering a specific input, such as increasing social advertising spend, on the outcome variable (i.e. sales). This causal understanding is paramount for effective budget optimization.\n",
    "\n",
    "However, standard modeling procedures, particularly those involving selection based on statistical significance, prioritize in-sample fit over robustness and causal validity. This focus inadvertently introduces substantial biases and instability. When models that systematically misrepresent causal effects guide budget decisions, organizations face significant resource misallocation that directly impacts profitability. The aggregated and time-series nature of marketing data further compounds these challenges, as weekly or monthly observations limit statistical power while still requiring controls for seasonality, economic trends, and competitive actions\n",
    "\n",
    "As marketing analytics continues to drive increasingly larger investment decisions, the potential damage from flawed model specifications grows proportionally. This page examines how traditional variable selection approaches might generate misleading insights and offers a more robust Bayesian framework for addressing these methodological pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Statistical Challenges in MMM Selection\n",
    "The selection of factors and transformations in MMM faces a confluence of statistical difficulties:\n",
    "\n",
    "- **Multiple Testing Problem:** Simultaneously testing numerous potential external factors and functional forms for media variables drastically increases the probability of false positives (Type I errors). Standard significance thresholds (e.g., p < 0.05) are inadequate for controlling the overall error rate across such extensive testing.\n",
    "\n",
    "- **Post-Selection Inference (PSI) Invalidity:** Standard statistical inference (p-values, confidence intervals) lacks theoretical validity when applied after model components have been selected using the same data. These post-selection metrics are often biased (e.g., p-values too small, confidence intervals too narrow), failing to maintain their nominal error rates or coverage probabilities.\n",
    "\n",
    "- **Time-Series Complications:** MMM data's time-series nature introduces further challenges:\n",
    "\n",
    "    - *Autocorrelation:* Residuals are often correlated over time, violating standard regression assumptions and rendering OLS p-values inaccurate.\n",
    "\n",
    "    - *Non-stationarity:* Trends in variables can lead to spurious regression—finding significant relationships where none exist causally.\n",
    "\n",
    "    - *Seasonality:* Unaccounted-for seasonal patterns can induce spurious correlations. These properties can invalidate the significance tests used during selection, compounding the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Testing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Selection Inference (PSI) Invalidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Series Complications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Problematic Selection Practices in MMM\n",
    "Several widely used methodologies are particularly susceptible to the challenges outlined above:\n",
    "\n",
    "- **Stepwise Regression:** Automated procedures (forward, backward, bidirectional) iteratively add/remove variables based on significance thresholds. These are highly prone to multiple testing errors and instability, especially with autocorrelated or non-stationary time-series data, potentially selecting variables based on spurious correlations or invalid tests.\n",
    "\n",
    "- **Significance-Based Transformation Selection:** Testing numerous functional forms (e.g., hundreds of adstock decays or saturation curves) for each media variable and selecting the one yielding the \"most significant\" coefficient represents a severe form of multiple testing, risking the selection of forms that fit noise.\n",
    "\n",
    "- **Correlation Screening / Univariate Tests:** Selecting external factors based on simple correlations or individual significance tests ignores multicollinearity and is highly vulnerable to spurious findings driven by trends or seasonality, while also failing to account for multiple comparisons.\n",
    "\n",
    "- **Best Subset Selection:** While potentially better if using information criteria (AIC/BIC), evaluating numerous model subsets still incurs a multiple testing burden and remains vulnerable to underlying time-series issues if not properly addressed within the model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consequences of Flawed Selection in MMM\n",
    "Employing naive selection methods and subsequent inference leads to significant practical problems:\n",
    "\n",
    "- **Misattribution of ROI & Effects:** Biased estimates for marketing coefficients (ROI) and potentially inaccurate representations of media dynamics (adstock, saturation).\n",
    "\n",
    "- **Flawed Budget Allocation:** Suboptimal marketing investment decisions stemming from unreliable ROI figures.\n",
    "\n",
    "- **Poor Understanding of Business Drivers:** Incorrect identification of baseline factors (trends, seasonality, macroeconomics) and media response patterns.\n",
    "\n",
    "- **Model Instability & Non-Reproducibility:** Selected factors and transformations may vary considerably with data updates, reducing model credibility.\n",
    "\n",
    "- **Overfitting:** Models capture noise specific to the historical data, resulting in poor predictive performance for forecasting or simulations.\n",
    "\n",
    "- **Misinterpretation of Control Factor Coefficients:** Attributing causal effects to the coefficients of baseline or control factors (e.g., macroeconomic variables, competitor activity) included in the model. These factors are typically observational and likely confounded themselves; their coefficients primarily reflect statistical association and adjustment needed to isolate media effects, not necessarily isolated causal impacts. This misinterpretation is related to the \"Table 2 fallacy,\" where coefficients from a multivariable model are improperly treated as independent causal effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Approaches and Considerations\n",
    "Addressing these challenges requires more robust methodologies:\n",
    "\n",
    "- **Rigorous Time-Series Handling:** Explicitly model or remove seasonality (e.g., dummies, Fourier terms, decomposition); test for and address non-stationarity (e.g., differencing); incorporate theoretically sound lags for media (adstock) and potentially external variables.\n",
    "\n",
    "- **Regularization Methods (LASSO, Ridge, Elastic Net):** Handle many predictors simultaneously, perform coefficient shrinkage and implicit variable selection, often yielding more stable results than stepwise methods. Must be applied in conjunction with appropriate time-series structures.\n",
    "\n",
    "- **Information Criteria (AIC, BIC):** Use for comparing non-nested models that correctly account for time-series properties, providing a more principled approach than p-value thresholds alone.\n",
    "\n",
    "- **Time-Series Cross-Validation:** Employ methods like rolling-origin validation to assess out-of-sample predictive performance robustly.\n",
    "\n",
    "- **Bayesian Frameworks:** Offer a probabilistic approach to uncertainty.\n",
    "\n",
    "    - *Priors on Functional Forms:* Incorporate prior knowledge or average over plausible media transformations (adstock/saturation) instead of hard selection.\n",
    "\n",
    "    - *Sparsity-Inducing Priors (e.g., Regularized Horseshoe):* Provide principled variable selection for external factors by shrinking irrelevant coefficients while retaining influential ones, directly modeling inclusion uncertainty.\n",
    "\n",
    "- **Causal Inference Techniques:** Explore advanced time-series methods if the primary goal is establishing causal links (use with caution).\n",
    "\n",
    "- **Domain Knowledge & Theory:** Prioritize pre-selecting candidate factors and transformation ranges based on business logic, economic theory, and prior research. Validate final model components for plausibility and stability.\n",
    "\n",
    "::: {.callout-important}\n",
    "## Improper Use of Domain Knowledge\n",
    "Exogenous factors in MMM are frequently confounded with media variables or other unobserved drivers. Consequently, the estimated coefficient for an exogenous variable may not represent its direct causal impact on the outcome but rather the statistical adjustment necessary to deconfound the estimated media effects. Rejecting such a variable because its coefficient sign contradicts simple causal expectations might inadvertently remove a necessary control variable, potentially leading to more biased estimates of media effectiveness.\n",
    ":::\n",
    "Adopting these more rigorous approaches is fundamental to developing media mix models that are statistically sound, reliable, and strategically valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
