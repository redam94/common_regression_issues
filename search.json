[
  {
    "objectID": "model_helpers/fourier_components.html",
    "href": "model_helpers/fourier_components.html",
    "title": "Fourier Components",
    "section": "",
    "text": "source\n\ngenerate_fourier_components\n\n generate_fourier_components (N_samples:int, N_components:int=3,\n                              period:int=52)\n\nGenerate Fourier components for a given number of samples and components.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN_samples\nint\n\nNumber of samples\n\n\nN_components\nint\n3\nHalf the number of Fourier components\n\n\nperiod\nint\n52\nYearly period of weekly data\n\n\nReturns\nDataFrame\n\nDataframe with fourier components\n\n\n\n\nperiod = pd.date_range(\"2021-01-01\", periods=156, freq=\"W-MON\")\nfourier_components = generate_fourier_components(period.shape[0], 3, 52)\nfourier_components.head()\n\n\n\n\n\n\n\n\nfourier_sin_0\nfourier_sin_1\nfourier_sin_2\nfourier_cos_0\nfourier_cos_1\nfourier_cos_2\n\n\n\n\n0\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\n1\n0.121311\n0.240829\n0.356791\n0.992615\n0.970568\n0.934184\n\n\n2\n0.240829\n0.467482\n0.666616\n0.970568\n0.884003\n0.745401\n\n\n3\n0.356791\n0.666616\n0.888695\n0.934184\n0.745401\n0.458499\n\n\n4\n0.467482\n0.826511\n0.993793\n0.884003\n0.562921\n0.111245\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Fourier components.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Samples of linear combination of fourier components.",
    "crumbs": [
      "model_helpers",
      "Fourier Components"
    ]
  },
  {
    "objectID": "multicollinearity.html",
    "href": "multicollinearity.html",
    "title": "When is Multicollinearity an Issue?",
    "section": "",
    "text": "Multicollinearity arises when independent variables in a regression model are highly correlated, leading to challenges in estimating individual effects accurately. This issue can inflate standard errors, reduce the precision of coefficient estimates, and complicate the interpretation of results. Understanding when multicollinearity becomes problematic is crucial for developing robust regression models. This section explores the conditions under which multicollinearity affects model performance, discusses its implications, and provides strategies for detection and mitigation.",
    "crumbs": [
      "When is Multicollinearity an Issue?"
    ]
  },
  {
    "objectID": "multicollinearity.html#the-dag",
    "href": "multicollinearity.html#the-dag",
    "title": "When is Multicollinearity an Issue?",
    "section": "The DAG",
    "text": "The DAG\n\n\n\n\n\n\n\n\nPaid Media on Sales\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nDemand\n\nDemand\n\n\n\nSeasonality-&gt;Demand\n\n\n\n\n\nOLV Impression\n\nOLV Impression\n\n\n\nSeasonality-&gt;OLV Impression\n\n\n\n\n\nSocial Impression\n\nSocial Impression\n\n\n\nSeasonality-&gt;Social Impression\n\n\n\n\n\nSearch Query\n\nSearch Query\n\n\n\nSeasonality-&gt;Search Query\n\n\n\n\n\nDemand-&gt;Search Query\n\n\n\n\n\nPaid Search Click\n\nPaid Search Click\n\n\n\nDemand-&gt;Paid Search Click\n\n\n\n\n\nSales\n\nSales\n\n\n\nDemand-&gt;Sales\n\n\n\n\n\nAuction\n\nAuction\n\n\n\nPaid Search Impression\n\nPaid Search Impression\n\n\n\nAuction-&gt;Paid Search Impression\n\n\n\n\n\nOLV Impression-&gt;Demand\n\n\n\n\n\nVideo Platform Sentiment\n\nVideo Platform Sentiment\n\n\n\nVideo Platform Sentiment-&gt;OLV Impression\n\n\n\n\n\nSocial Impression-&gt;Demand\n\n\n\n\n\nSearch Query-&gt;Auction\n\n\n\n\n\nSearch Query-&gt;Paid Search Impression\n\n\n\n\n\nSearch Query-&gt;Paid Search Click\n\n\n\n\n\nOrganic Search\n\nOrganic Search\n\n\n\nSearch Query-&gt;Organic Search\n\n\n\n\n\nPaid Search Impression-&gt;Paid Search Click\n\n\n\n\n\nPaid Search Click-&gt;Sales\n\n\n\n\n\nOrganic Search-&gt;Sales\n\n\n\n\n\nPrice\n\nPrice\n\n\n\nPrice-&gt;Demand\n\n\n\n\n\nPrice-&gt;Sales\n\n\n\n\n\n\n\n\nFigure 1: The causal graph reflects our assumptions about the data-generating process in this example. This model may need refinement, so it’s essential to review it with experts and stakeholders to enhance its structure and incorporate additional factors that could influence the variables of interest.\n\n\n\n\n\n\n\n\n\n\n\nHow to read a DAG\n\n\n\n\n\nA Directed Acyclic Graph (DAG) serves as a model of causality, where directional relationships between nodes indicate that causes flow from one node to another along the arrows, forming a distinct pathway of influence. Each arrow represents a causal link, signaling how changes in one node could potentially impact another. In a DAG, the relationships between nodes (depicted by an arrow) are not restricted to being linear; they can incorporate complex, non-linear interactions.\nEdges will be color-coded to emphasize whether they represent causal paths (green ), biasing paths (red ), or non-causal paths (black ). Refer to Figure 2 and Figure 3.\nNodes indicating exposures (factors for which we want to understand the effect) will have a green background (). The outcome variable of interest will have a blue background (), while variables adjusted for will have a light gray background (). Nodes with a dashed outline represent elements that are challenging or impossible to observe directly, while nodes with a solid outline and no fill represent variables for which data is available or can be acquired.\n\n\n\n\nHow Effective is Paid Search at Driving Sales?\nTo accurately measure the impact of Paid Search on Sales, we must carefully consider the causal pathways influencing this relationship. The causal model presented in Figure 1 reveals several open biasing paths from both paid search impressions and paid search clicks to sales. These open paths introduce potential sources of bias that can distort our understanding of Paid Search’s effectiveness.\nAdditionally, the DAG highlights a complex correlation structure within the data, which suggests that variables may be highly interrelated. This correlation can lead to high Variance Inflation Factors (VIFs) in a regression model.\nElevated VIFs inflate standard errors and reduce the precision of our estimates, but excluding the wrong variables could result in biased estimates. Balancing these considerations—minimizing multicollinearity without overlooking key variables—is essential for creating a model that yields accurate and precise estimates of paid search’s impact on sales.\n\nSolution\nTo get an unbiased estimate of the effect of paid search impressions on sale, given the bias introduced by demand (an unobserved variable), focusing on search query as an adjustment variable is a sound approach. Here’s a breakdown of the reasoning and approach based on the DAG structure:\n\nDirect Path and Biasing Path:\n\nPaid search clicks have a direct impact on sales, but due to demand (which is unobserved and thus unadjustable in this analysis), there’s a risk of confounding.\nSince demand also influences search query, which in turn influences both paid search impressions and paid search clicks, search query becomes a potential control variable to block the non-causal path through demand. See Figure 2\n\nAdjustment for Search Query:\n\nBy adjusting for search query, we aim to block the path from demand that leads through search query to both paid search impressions and paid search clicks. This adjustment helps remove the confounding influence of demand on sales via search query, allowing for a more accurate estimate of paid search impressions’ effect on sales. See Figure 3\n\nOutcome:\n\nAdjusting for search query should allow for a clearer view of the total effect of paid search impressions on sales by blocking the biasing path from demand, leading to a more reliable analysis.\n\n\nIn summary, adjusting for search query effectively helps control for the unobserved demand factor, allowing us to examine the impact of paid search impressions on sales with minimized bias. We can also increase the precision of this estimate by controlling for price.\n\nUn-Adjusted ModelAdjusted Model\n\n\n\n\n\n\n\n\n\n\nPaid Media on Sales\n\n\n\nPaid Search Impression\n\nPaid Search Impression\n\n\n\nPaid Search Click\n\nPaid Search Click\n\n\n\nPaid Search Impression-&gt;Paid Search Click\n\n\n\n\n\nSales\n\nSales\n\n\n\nOLV Impression\n\nOLV Impression\n\n\n\nDemand\n\nDemand\n\n\n\nOLV Impression-&gt;Demand\n\n\n\n\n\nSocial Impression\n\nSocial Impression\n\n\n\nSocial Impression-&gt;Demand\n\n\n\n\n\nVideo Platform Sentiment\n\nVideo Platform Sentiment\n\n\n\nVideo Platform Sentiment-&gt;OLV Impression\n\n\n\n\n\nSearch Query\n\nSearch Query\n\n\n\nSearch Query-&gt;Paid Search Impression\n\n\n\n\n\nSearch Query-&gt;Paid Search Click\n\n\n\n\n\nOrganic Search\n\nOrganic Search\n\n\n\nSearch Query-&gt;Organic Search\n\n\n\n\n\nAuction\n\nAuction\n\n\n\nSearch Query-&gt;Auction\n\n\n\n\n\nPaid Search Click-&gt;Sales\n\n\n\n\n\nPrice\n\nPrice\n\n\n\nPrice-&gt;Sales\n\n\n\n\n\nPrice-&gt;Demand\n\n\n\n\n\nOrganic Search-&gt;Sales\n\n\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nSeasonality-&gt;OLV Impression\n\n\n\n\n\nSeasonality-&gt;Social Impression\n\n\n\n\n\nSeasonality-&gt;Search Query\n\n\n\n\n\nSeasonality-&gt;Demand\n\n\n\n\n\nDemand-&gt;Sales\n\n\n\n\n\nDemand-&gt;Search Query\n\n\n\n\n\nDemand-&gt;Paid Search Click\n\n\n\n\n\nAuction-&gt;Paid Search Impression\n\n\n\n\n\n\n\n\nFigure 2: We aim to understand the effect of Paid Search Impressions on Sales. Without any adjustments, all biasing paths remain open. To accurately estimate this effect, we need to adjust for variables that close these biasing paths and include additional variables to reduce variance in our estimate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaid Media on Sales\n\n\n\nPaid Search Impression\n\nPaid Search Impression\n\n\n\nPaid Search Click\n\nPaid Search Click\n\n\n\nPaid Search Impression-&gt;Paid Search Click\n\n\n\n\n\nSales\n\nSales\n\n\n\nPrice\n\nPrice\n\n\n\nPrice-&gt;Sales\n\n\n\n\n\nDemand\n\nDemand\n\n\n\nPrice-&gt;Demand\n\n\n\n\n\nSearch Query\n\nSearch Query\n\n\n\nSearch Query-&gt;Paid Search Impression\n\n\n\n\n\nSearch Query-&gt;Paid Search Click\n\n\n\n\n\nOrganic Search\n\nOrganic Search\n\n\n\nSearch Query-&gt;Organic Search\n\n\n\n\n\nAuction\n\nAuction\n\n\n\nSearch Query-&gt;Auction\n\n\n\n\n\nOLV Impression\n\nOLV Impression\n\n\n\nOLV Impression-&gt;Demand\n\n\n\n\n\nSocial Impression\n\nSocial Impression\n\n\n\nSocial Impression-&gt;Demand\n\n\n\n\n\nVideo Platform Sentiment\n\nVideo Platform Sentiment\n\n\n\nVideo Platform Sentiment-&gt;OLV Impression\n\n\n\n\n\nPaid Search Click-&gt;Sales\n\n\n\n\n\nOrganic Search-&gt;Sales\n\n\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nSeasonality-&gt;Sales\n\n\n\n\n\nSeasonality-&gt;Search Query\n\n\n\n\n\nSeasonality-&gt;OLV Impression\n\n\n\n\n\nSeasonality-&gt;Social Impression\n\n\n\n\n\nSeasonality-&gt;Demand\n\n\n\n\n\nDemand-&gt;Sales\n\n\n\n\n\nDemand-&gt;Search Query\n\n\n\n\n\nDemand-&gt;Paid Search Click\n\n\n\n\n\nAuction-&gt;Paid Search Impression\n\n\n\n\n\n\n\n\nFigure 3: Adjusting for Search Query closes all biasing paths. Additionally, Price is adjusted, not to close biasing paths, but to reduce the variance in estimating the effect of Paid Search Impressions on Sales.\n\n\n\n\n\n\n\n\n\nCorrelation MatrixVIFs\n\n\n\n\n\n\n\n\n\n\n\n \npaid_search_impressions\nsearch_query\nprice\nsales\n\n\n\n\npaid_search_impressions\n1.00\n0.93\n-0.34\n0.90\n\n\nsearch_query\n0.93\n1.00\n-0.33\n0.91\n\n\nprice\n-0.34\n-0.33\n1.00\n-0.19\n\n\nsales\n0.90\n0.91\n-0.19\n1.00\n\n\n\n\n\nFigure 4: Correlation matrix\n\n\n\n\n\n\n\n\n\nTable 1: VIF for the regression model\n\n\n\n                           VIF\npaid_search_impressions  : 1667.63\nsearch_query             : 1952.48\nprice                    : 72.44\n\n\n\n\n\n\n\nWith our adjustment set established, let’s examine the correlation structure among the key variables (paid search impressions, search query, price, and sales). As shown in Figure 4, search query and paid search impressions exhibit a high correlation. Additionally, the VIF values presented in Table 1 reveal some extreme figures.\nRather than being immediately concerned about the high VIFs (even those exceeding 1000), let’s proceed by running the regression to assess the actual impact.\nIn Table 3, the standard errors for the coefficients are reasonably sized. Since this is synthetic data, we can compare the estimated coefficient for paid search impressions to the expected value of 0.4, which indeed falls within the confidence interval for this coefficient.\nIn Table 2, where search query is excluded from the model, we observe a reduction in standard errors. However, this reduction comes at the expense of introducing bias into the estimate of paid search impressions’ effect on sales. In fact it produces an estimate that is much larger than the true value.\nGiven that the correctly adjusted model provides an estimate with reasonable precision, it should be preferred, even in light of the high VIFs.\n\nUn-Adjusted ModelAdjusted Model\n\n\n\n\n\n\nTable 2: The unadjusted regression of the form \\[log(Sales) \\sim \\beta_{psi} log(Paid Search Impressions) + \\beta_p log(Price)\\]\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsales\nR-squared:\n0.832\n\n\nModel:\nOLS\nAdj. R-squared:\n0.830\n\n\nMethod:\nLeast Squares\nF-statistic:\n379.1\n\n\nDate:\nTue, 19 Nov 2024\nProb (F-statistic):\n5.21e-60\n\n\nTime:\n19:17:11\nLog-Likelihood:\n-27.949\n\n\nNo. Observations:\n156\nAIC:\n61.90\n\n\nDf Residuals:\n153\nBIC:\n71.05\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-2.0391\n0.532\n-3.835\n0.000\n-3.090\n-0.989\n\n\npaid_search_impressions\n0.8323\n0.031\n26.950\n0.000\n0.771\n0.893\n\n\nprice\n0.6034\n0.160\n3.779\n0.000\n0.288\n0.919\n\n\n\n\n\n\n\n\nOmnibus:\n1.250\nDurbin-Watson:\n1.504\n\n\nProb(Omnibus):\n0.535\nJarque-Bera (JB):\n0.862\n\n\nSkew:\n0.042\nProb(JB):\n0.650\n\n\nKurtosis:\n3.354\nCond. No.\n299.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\nTable 3: The correctly adjusted regression model of the following form \\[log(Sales) \\sim \\beta_{SQ} log(Search Query) + \\beta_{psi} log(Paid Search Impressions) + \\beta_p log(Price)\\]\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsales\nR-squared:\n0.867\n\n\nModel:\nOLS\nAdj. R-squared:\n0.865\n\n\nMethod:\nLeast Squares\nF-statistic:\n331.6\n\n\nDate:\nTue, 19 Nov 2024\nProb (F-statistic):\n1.82e-66\n\n\nTime:\n19:17:12\nLog-Likelihood:\n-9.4959\n\n\nNo. Observations:\n156\nAIC:\n26.99\n\n\nDf Residuals:\n152\nBIC:\n39.19\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-4.1601\n0.579\n-7.182\n0.000\n-5.305\n-3.016\n\n\npaid_search_impressions\n0.4287\n0.069\n6.205\n0.000\n0.292\n0.565\n\n\nsearch_query\n0.4739\n0.074\n6.369\n0.000\n0.327\n0.621\n\n\nprice\n0.6448\n0.142\n4.526\n0.000\n0.363\n0.926\n\n\n\n\n\n\n\n\nOmnibus:\n0.507\nDurbin-Watson:\n1.777\n\n\nProb(Omnibus):\n0.776\nJarque-Bera (JB):\n0.214\n\n\nSkew:\n-0.035\nProb(JB):\n0.899\n\n\nKurtosis:\n3.167\nCond. No.\n559.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\nWhen High VIFs are an issue\nWhile high VIFs are not inherently problematic, they serve as a useful diagnostic tool for identifying potential issues in regression results. Let’s explore this further with a slightly more complex example.\n\n\nHow effective is OLV at driving sales?\nFrom the causal model, we can see that seasonality is part of the necessary adjustment set to accurately estimate the effect of OLV impressions on sales. Although seasonality is unobserved, we can reasonably approximate it using a periodic function (e.g., sine or cosine terms) to capture seasonal fluctuations over time. Let’s consider the following model were we adjust for seasonality (using fourier terms), price, olv impressions, and Video Platform Sentiment (olv_sentiment in the models). Note that Video Platform Sentiment is not a confounder, typically we would not adjust for it, but we include it here to illustrate the point that adjusting for variables that are not confounders can needlessly increase VIFs.\n\nOver Adjusted ModelCorrectly Adjusted Model\n\n\n\n\n\n\n\n\n\n\nPaid Media on Sales\n\n\n\nOLV Impression\n\nOLV Impression\n\n\n\nDemand\n\nDemand\n\n\n\nOLV Impression-&gt;Demand\n\n\n\n\n\nSales\n\nSales\n\n\n\nPrice\n\nPrice\n\n\n\nPrice-&gt;Sales\n\n\n\n\n\nPrice-&gt;Demand\n\n\n\n\n\nVideo Platform Sentiment\n\nVideo Platform Sentiment\n\n\n\nVideo Platform Sentiment-&gt;OLV Impression\n\n\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nSeasonality-&gt;OLV Impression\n\n\n\n\n\nSeasonality-&gt;Sales\n\n\n\n\n\nSearch Query\n\nSearch Query\n\n\n\nSeasonality-&gt;Search Query\n\n\n\n\n\nSocial Impression\n\nSocial Impression\n\n\n\nSeasonality-&gt;Social Impression\n\n\n\n\n\nSeasonality-&gt;Demand\n\n\n\n\n\nPaid Search Impression\n\nPaid Search Impression\n\n\n\nSearch Query-&gt;Paid Search Impression\n\n\n\n\n\nPaid Search Click\n\nPaid Search Click\n\n\n\nSearch Query-&gt;Paid Search Click\n\n\n\n\n\nOrganic Search\n\nOrganic Search\n\n\n\nSearch Query-&gt;Organic Search\n\n\n\n\n\nAuction\n\nAuction\n\n\n\nSearch Query-&gt;Auction\n\n\n\n\n\nPaid Search Impression-&gt;Paid Search Click\n\n\n\n\n\nSocial Impression-&gt;Demand\n\n\n\n\n\nPaid Search Click-&gt;Sales\n\n\n\n\n\nOrganic Search-&gt;Sales\n\n\n\n\n\nDemand-&gt;Sales\n\n\n\n\n\nDemand-&gt;Search Query\n\n\n\n\n\nDemand-&gt;Paid Search Click\n\n\n\n\n\nAuction-&gt;Paid Search Impression\n\n\n\n\n\n\n\n\nFigure 5: The causal graph for the relationship between OLV Impressions and Sales. Adjusting for Seasonality and Price closes all biasing paths, allowing for an unbiased estimate of the effect of OLV Impressions on Sales. Including Video Platform Sentement does nothing to reduce bias and only adds variance of our estimate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaid Media on Sales\n\n\n\nOLV Impression\n\nOLV Impression\n\n\n\nDemand\n\nDemand\n\n\n\nOLV Impression-&gt;Demand\n\n\n\n\n\nSales\n\nSales\n\n\n\nPrice\n\nPrice\n\n\n\nPrice-&gt;Sales\n\n\n\n\n\nPrice-&gt;Demand\n\n\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nSeasonality-&gt;OLV Impression\n\n\n\n\n\nSeasonality-&gt;Sales\n\n\n\n\n\nSearch Query\n\nSearch Query\n\n\n\nSeasonality-&gt;Search Query\n\n\n\n\n\nSocial Impression\n\nSocial Impression\n\n\n\nSeasonality-&gt;Social Impression\n\n\n\n\n\nSeasonality-&gt;Demand\n\n\n\n\n\nPaid Search Impression\n\nPaid Search Impression\n\n\n\nSearch Query-&gt;Paid Search Impression\n\n\n\n\n\nPaid Search Click\n\nPaid Search Click\n\n\n\nSearch Query-&gt;Paid Search Click\n\n\n\n\n\nOrganic Search\n\nOrganic Search\n\n\n\nSearch Query-&gt;Organic Search\n\n\n\n\n\nAuction\n\nAuction\n\n\n\nSearch Query-&gt;Auction\n\n\n\n\n\nPaid Search Impression-&gt;Paid Search Click\n\n\n\n\n\nSocial Impression-&gt;Demand\n\n\n\n\n\nVideo Platform Sentiment\n\nVideo Platform Sentiment\n\n\n\nVideo Platform Sentiment-&gt;OLV Impression\n\n\n\n\n\nPaid Search Click-&gt;Sales\n\n\n\n\n\nOrganic Search-&gt;Sales\n\n\n\n\n\nDemand-&gt;Sales\n\n\n\n\n\nDemand-&gt;Search Query\n\n\n\n\n\nDemand-&gt;Paid Search Click\n\n\n\n\n\nAuction-&gt;Paid Search Impression\n\n\n\n\n\n\n\n\nFigure 6: Adjusting for Seasonality and Price closes all biasing paths, allowing for an unbiased estimate of the effect of OLV Impressions on Sales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFourier Seasonality\n\n\n\n\n\nTo capture the seasonal fluctuations in the data, we can use Fourier terms to approximate the unobserved seasonality variable. These terms are periodic functions that can model the cyclical patterns in the data, allowing us to adjust for the influence of seasonality on sales and OLV impressions. By including these terms in the regression model, we can effectively control for the confounding effect of seasonality and obtain an unbiased estimate of the effect of OLV impressions on sales.\nI will use a model helper to generated the fourier\n\n\n\n\n\n\n\n\nFigure 7: Fourier seasonality\n\n\n\n\n\n\n\n\n\nOver Adjusted ModelCorrectly Adjusted Model\n\n\n\n\n\nTable 4: VIF for the regression model with Fourier components with the addition of OLV sentiment\n\n\n\n               VIF\nhill_olv     : 70.91\nlog_price    : 57.59\nolv_sentiment: 4.43\nfourier_sin_0: 1.01\nfourier_sin_1: 1.01\nfourier_sin_2: 1.0\nfourier_cos_0: 1.42\nfourier_cos_1: 1.01\nfourier_cos_2: 1.01\n\n\n\n\n\n\n\n\n\nTable 5: VIF for the regression model with Fourier components without the addition of OLV sentiment\n\n\n\n               VIF\nhill_olv     : 18.98\nlog_price    : 18.84\nfourier_sin_0: 1.01\nfourier_sin_1: 1.0\nfourier_sin_2: 1.0\nfourier_cos_0: 1.14\nfourier_cos_1: 1.0\nfourier_cos_2: 1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nVectors Spaces and Basis Vectors\n\n\n\n\n\nFourier components provide an orthogonal basis for the space of periodic functions with a given period \\(L\\) . This orthogonality helps to address multicollinearity, which is why the VIFs in Table 5 and Table 4 are all near 1 for the fourier components!\n\n\n\n\nOver Adjusted ModelCorrectly Adjusted Model\n\n\n\n\n\n\nTable 6: The over adjusted regression model with Fourier components with the addition of OLV sentiment\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsales\nR-squared:\n0.888\n\n\nModel:\nOLS\nAdj. R-squared:\n0.881\n\n\nMethod:\nLeast Squares\nF-statistic:\n128.3\n\n\nDate:\nTue, 19 Nov 2024\nProb (F-statistic):\n9.51e-65\n\n\nTime:\n19:17:22\nLog-Likelihood:\n3.4816\n\n\nNo. Observations:\n156\nAIC:\n13.04\n\n\nDf Residuals:\n146\nBIC:\n43.54\n\n\nDf Model:\n9\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n10.7020\n0.324\n32.982\n0.000\n10.061\n11.343\n\n\nhill_olv\n0.7217\n0.433\n1.665\n0.098\n-0.135\n1.578\n\n\nlog_price\n-1.0720\n0.127\n-8.433\n0.000\n-1.323\n-0.821\n\n\nolv_sentiment\n0.0183\n0.161\n0.114\n0.909\n-0.299\n0.336\n\n\nfourier_sin_0\n0.0449\n0.028\n1.591\n0.114\n-0.011\n0.101\n\n\nfourier_sin_1\n0.0214\n0.028\n0.768\n0.444\n-0.034\n0.077\n\n\nfourier_sin_2\n0.0166\n0.028\n0.596\n0.552\n-0.038\n0.072\n\n\nfourier_cos_0\n-0.8321\n0.041\n-20.141\n0.000\n-0.914\n-0.750\n\n\nfourier_cos_1\n0.1959\n0.028\n6.963\n0.000\n0.140\n0.251\n\n\nfourier_cos_2\n0.0084\n0.028\n0.303\n0.763\n-0.046\n0.063\n\n\n\n\n\n\n\n\nOmnibus:\n0.997\nDurbin-Watson:\n1.689\n\n\nProb(Omnibus):\n0.607\nJarque-Bera (JB):\n0.912\n\n\nSkew:\n-0.186\nProb(JB):\n0.634\n\n\nKurtosis:\n2.964\nCond. No.\n53.9\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\nTable 7: The correctly adjusted regression model with Fourier components and no OLV sentiment\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsales\nR-squared:\n0.888\n\n\nModel:\nOLS\nAdj. R-squared:\n0.882\n\n\nMethod:\nLeast Squares\nF-statistic:\n145.4\n\n\nDate:\nTue, 19 Nov 2024\nProb (F-statistic):\n7.68e-66\n\n\nTime:\n19:18:03\nLog-Likelihood:\n3.4747\n\n\nNo. Observations:\n156\nAIC:\n11.05\n\n\nDf Residuals:\n147\nBIC:\n38.50\n\n\nDf Model:\n8\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n10.6756\n0.226\n47.201\n0.000\n10.229\n11.123\n\n\nhill_olv\n0.7678\n0.157\n4.898\n0.000\n0.458\n1.078\n\n\nlog_price\n-1.0712\n0.126\n-8.468\n0.000\n-1.321\n-0.821\n\n\nfourier_sin_0\n0.0444\n0.028\n1.597\n0.112\n-0.011\n0.099\n\n\nfourier_sin_1\n0.0215\n0.028\n0.773\n0.441\n-0.033\n0.076\n\n\nfourier_sin_2\n0.0167\n0.028\n0.603\n0.548\n-0.038\n0.071\n\n\nfourier_cos_0\n-0.8289\n0.030\n-27.407\n0.000\n-0.889\n-0.769\n\n\nfourier_cos_1\n0.1953\n0.028\n7.081\n0.000\n0.141\n0.250\n\n\nfourier_cos_2\n0.0086\n0.028\n0.313\n0.755\n-0.046\n0.063\n\n\n\n\n\n\n\n\nOmnibus:\n1.098\nDurbin-Watson:\n1.688\n\n\nProb(Omnibus):\n0.577\nJarque-Bera (JB):\n0.987\n\n\nSkew:\n-0.195\nProb(JB):\n0.610\n\n\nKurtosis:\n2.979\nCond. No.\n26.6\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: The correctly adjusted regression model with Fourier components and no OLV sentiment\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsales\nR-squared:\n0.063\n\n\nModel:\nOLS\nAdj. R-squared:\n0.051\n\n\nMethod:\nLeast Squares\nF-statistic:\n5.166\n\n\nDate:\nTue, 19 Nov 2024\nProb (F-statistic):\n0.00674\n\n\nTime:\n19:27:23\nLog-Likelihood:\n-162.03\n\n\nNo. Observations:\n156\nAIC:\n330.1\n\n\nDf Residuals:\n153\nBIC:\n339.2\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n10.1355\n0.637\n15.919\n0.000\n8.878\n11.393\n\n\nlog_price\n-0.8167\n0.355\n-2.299\n0.023\n-1.519\n-0.115\n\n\nhill_olv_predicted\n0.9943\n0.463\n2.148\n0.033\n0.080\n1.909\n\n\n\n\n\n\n\n\nOmnibus:\n23.961\nDurbin-Watson:\n0.227\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n11.734\n\n\nSkew:\n0.487\nProb(JB):\n0.00283\n\n\nKurtosis:\n2.075\nCond. No.\n26.5\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nHere, we observe that although the VIF values are considerably lower than those reported in Table 1 in the previous example (1.1.1), the impact on our ability to accurately measure the effect of OLV impressions on sales is significantly more detrimental.\nComparing Table 7 to Table 6 we see that the inclusion of Video Platform Sentiment (represented as olv_sentement in the model), did not bias the estimate of the total effect of OLV impressions on sales. However, it did lead to an increase in p-values, pushing them above our significance threshold of 0.05.",
    "crumbs": [
      "When is Multicollinearity an Issue?"
    ]
  },
  {
    "objectID": "multicollinearity.html#appendix-a---paid-search-data",
    "href": "multicollinearity.html#appendix-a---paid-search-data",
    "title": "When is Multicollinearity an Issue?",
    "section": "Appendix A - Paid Search Data",
    "text": "Appendix A - Paid Search Data\n\nSalesPaid Search ImpressionsSearch Query VolumePrice",
    "crumbs": [
      "When is Multicollinearity an Issue?"
    ]
  },
  {
    "objectID": "multicollinearity.html#appendix-b---olv-data",
    "href": "multicollinearity.html#appendix-b---olv-data",
    "title": "When is Multicollinearity an Issue?",
    "section": "Appendix B - OLV Data",
    "text": "Appendix B - OLV Data\n\nSalesOLV ImpressionsVideo Platform SentimentPrice",
    "crumbs": [
      "When is Multicollinearity an Issue?"
    ]
  },
  {
    "objectID": "normalization_in_panel_models.html",
    "href": "normalization_in_panel_models.html",
    "title": "Centering in Panel Models",
    "section": "",
    "text": "sales_demo_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 76kB\nDimensions:      (store_id: 20, time_period: 156, product_id: 1)\nCoordinates:\n  * store_id     (store_id) int64 160B 1 2 3 4 5 6 7 8 ... 14 15 16 17 18 19 20\n  * time_period  (time_period) int64 1kB 1 2 3 4 5 6 ... 151 152 153 154 155 156\n  * product_id   (product_id) int64 8B 1\nData variables:\n    sales        (store_id, time_period, product_id) float64 25kB 2.2 ... 10.04\n    covariate_1  (store_id, time_period, product_id) float64 25kB 0.4296 ... ...\n    covariate_2  (store_id, time_period, product_id) float64 25kB 0.6666 ... ...\nAttributes:\n    betas:                       [-0.01237172  0.07431322]\n    seasonality_amplitude:       0.22150897038028766\n    trend_slope:                 0.005464504127199977\n    store_effects:               [-0.44475205  0.12756087  0.11161652  0.4042...\n    base_log_sales_per_product:  [1.24908024]xarray.DatasetDimensions:store_id: 20time_period: 156product_id: 1Coordinates: (3)store_id(store_id)int641 2 3 4 5 6 7 ... 15 16 17 18 19 20array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 20])time_period(time_period)int641 2 3 4 5 6 ... 152 153 154 155 156array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n        15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n        43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n        57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n        71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n        85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n        99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n       127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n       141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n       155, 156])product_id(product_id)int641array([1])Data variables: (3)sales(store_id, time_period, product_id)float642.2 2.46 2.257 ... 7.872 8.87 10.04array([[[ 2.20014538],\n        [ 2.46006697],\n        [ 2.25713195],\n        ...,\n        [ 4.99957635],\n        [ 6.54083394],\n        [ 5.98004403]],\n\n       [[ 3.44366332],\n        [ 3.98512642],\n        [ 4.10876074],\n        ...,\n        [ 7.9728716 ],\n        [ 9.49967535],\n        [ 9.83488601]],\n\n       [[ 4.71521497],\n        [ 4.27953578],\n        [ 3.81123469],\n        ...,\n...\n        ...,\n        [ 8.74263671],\n        [ 8.43012539],\n        [10.71933388]],\n\n       [[ 9.16110797],\n        [11.47300089],\n        [11.29541857],\n        ...,\n        [20.25258712],\n        [22.9180807 ],\n        [21.40687927]],\n\n       [[ 3.87633769],\n        [ 3.76098006],\n        [ 4.49038221],\n        ...,\n        [ 7.87187451],\n        [ 8.87007886],\n        [10.04150868]]])covariate_1(store_id, time_period, product_id)float640.4296 -1.009 ... 0.2061 -0.2842array([[[ 0.42958216],\n        [-1.00886032],\n        [-1.63511595],\n        ...,\n        [-0.03117877],\n        [ 0.45840429],\n        [ 2.14985844]],\n\n       [[-0.0708224 ],\n        [-0.21249966],\n        [ 1.11349753],\n        ...,\n        [-0.92979536],\n        [ 0.50266762],\n        [-0.29202794]],\n\n       [[ 1.54458913],\n        [ 1.41137034],\n        [ 0.00548954],\n        ...,\n...\n        ...,\n        [-0.24282971],\n        [-2.123726  ],\n        [ 0.07961794]],\n\n       [[-1.80021537],\n        [ 1.16170186],\n        [-1.37146226],\n        ...,\n        [ 0.90555776],\n        [-0.79729777],\n        [-0.24175421]],\n\n       [[-0.2531813 ],\n        [ 0.09395003],\n        [-0.35201606],\n        ...,\n        [-0.88867211],\n        [ 0.20608888],\n        [-0.284199  ]]])covariate_2(store_id, time_period, product_id)float640.6666 -0.5858 ... 0.2851 0.2037array([[[ 0.66657458],\n        [-0.58575575],\n        [-1.39282621],\n        ...,\n        [-0.08985905],\n        [ 0.23471522],\n        [ 2.43593884]],\n\n       [[-0.00458811],\n        [-0.34196549],\n        [ 1.11175663],\n        ...,\n        [-1.21112759],\n        [ 0.48983728],\n        [-0.02517022]],\n\n       [[ 1.26184199],\n        [ 1.6979829 ],\n        [-0.27172106],\n        ...,\n...\n        ...,\n        [-0.17222864],\n        [-1.81783533],\n        [ 0.14365546]],\n\n       [[-1.47751046],\n        [ 1.50453944],\n        [-0.87181092],\n        ...,\n        [ 0.99883275],\n        [-0.84396878],\n        [-0.34967486]],\n\n       [[ 0.00729364],\n        [ 0.22927749],\n        [-0.29032389],\n        ...,\n        [-1.06890621],\n        [ 0.28512372],\n        [ 0.20369718]]])Indexes: (3)store_idPandasIndexPandasIndex(Index([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], dtype='int64', name='store_id'))time_periodPandasIndexPandasIndex(Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       147, 148, 149, 150, 151, 152, 153, 154, 155, 156],\n      dtype='int64', name='time_period', length=156))product_idPandasIndexPandasIndex(Index([1], dtype='int64', name='product_id'))Attributes: (5)betas :[-0.01237172  0.07431322]seasonality_amplitude :0.22150897038028766trend_slope :0.005464504127199977store_effects :[-0.44475205  0.12756087  0.11161652  0.40420611 -0.23235125 -0.21006792\n -0.22855207 -0.36963314 -1.04501961  0.38014787  0.32657803 -0.6095504\n -0.17121843 -0.29696273 -0.28133752 -0.85584826 -0.25178998  0.23908819\n  1.02379521  0.15769321]base_log_sales_per_product :[1.24908024]\n\n\n\ntime_index = sales_normed.time_period.values.astype(int)\n\nseasonal_control = np.sin(2 * np.pi * time_index / 52)\ntrend = time_index/52\n\n# Fit a linear regression model\nsales_df = sales_demo_data.to_dataframe().reset_index()\ncontrol_df = pd.DataFrame({\n    'seasonal_control': seasonal_control,\n    'trend': trend,\n    'time_period': time_index\n})\ncontrol_df['time_period'] = control_df['time_period']\n\ntotal_df = sales_df.merge(control_df, on='time_period',  how='left')\n\ntrain_df = total_df[total_df['time_period'] &lt; 104].copy().set_index(['store_id', 'time_period'])\ntest_df = total_df[total_df['time_period'] &gt;= 104].copy().set_index(['store_id', 'time_period'])\n\n# Creat the dependent variable and independent variables\nX_train = sm.add_constant(train_df[['seasonal_control', 'trend', 'covariate_1', 'covariate_2']])\ny_train = np.log(train_df['sales'])\ny_train_div = y_train/y_train.groupby('store_id').mean()\ny_train_sub = y_train-y_train.groupby('store_id').mean()\n\n# Fit the regression model\nME_model_standard = lm.RandomEffects(y_train, X_train)\nME_model_div = lm.RandomEffects(y_train_div, X_train)\nME_model_sub = lm.RandomEffects(y_train_sub, X_train)\n\n# Fit the model\nfitted_model_standard = ME_model_standard.fit()\nfitted_model_div = ME_model_div.fit()\nfitted_model_sub = ME_model_sub.fit()\n\n\nDiv ‘Normed’ ModelStandard MEMDependant Sub-NormalizedComparison of Normalization\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sales model fitted with div-normed dependent variable\n\n\n\n\n\n\n\n\n\n\n\nModel Summary\n\n\n\n\n\n\nfitted_model_div.summary\n\n\nRandomEffects Estimation Summary\n\n\nDep. Variable:\nsales\nR-squared:\n0.6325\n\n\nEstimator:\nRandomEffects\nR-squared (Between):\n-8.194e+27\n\n\nNo. Observations:\n2060\nR-squared (Within):\n0.6335\n\n\nDate:\nWed, May 21 2025\nR-squared (Overall):\n0.6325\n\n\nTime:\n21:57:24\nLog-likelihood\n1544.7\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n884.29\n\n\nEntities:\n20\nP-value\n0.0000\n\n\nAvg Obs:\n103.00\nDistribution:\nF(4,2055)\n\n\nMin Obs:\n103.00\n\n\n\n\nMax Obs:\n103.00\nF-statistic (robust):\n884.29\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n103\nDistribution:\nF(4,2055)\n\n\nAvg Obs:\n20.000\n\n\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nconst\n0.7644\n0.0054\n140.36\n0.0000\n0.7538\n0.7751\n\n\nseasonal_control\n0.1838\n0.0039\n47.554\n0.0000\n0.1762\n0.1913\n\n\ntrend\n0.2288\n0.0048\n47.656\n0.0000\n0.2194\n0.2383\n\n\ncovariate_1\n-0.0085\n0.0103\n-0.8255\n0.4092\n-0.0286\n0.0117\n\n\ncovariate_2\n0.0517\n0.0103\n5.0180\n0.0000\n0.0315\n0.0719\n\n\n\n\n\n\n(sales_demo_data.attrs['betas'][None,:]/y_train.groupby('store_id').mean().values.flatten()[:, None]).mean(axis=0)\n\narray([-0.00995315,  0.05978558])\n\n\n\nfitted_model_div.variance_decomposition\n\nEffects                   0.000000\nResidual                  0.013185\nPercent due to Effects    0.000000\nName: Variance Decomposition, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Sales model fitted with non-normed dependent variable\n\n\n\n\n\n\n\n\n\n\n\nModel summary\n\n\n\n\n\n\nfitted_model_standard.summary\n\n\nRandomEffects Estimation Summary\n\n\nDep. Variable:\nsales\nR-squared:\n0.7741\n\n\nEstimator:\nRandomEffects\nR-squared (Between):\n0.0074\n\n\nNo. Observations:\n2060\nR-squared (Within):\n0.7755\n\n\nDate:\nWed, May 21 2025\nR-squared (Overall):\n0.1414\n\n\nTime:\n21:57:24\nLog-likelihood\n1784.2\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n1760.2\n\n\nEntities:\n20\nP-value\n0.0000\n\n\nAvg Obs:\n103.00\nDistribution:\nF(4,2055)\n\n\nMin Obs:\n103.00\n\n\n\n\nMax Obs:\n103.00\nF-statistic (robust):\n1760.2\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n103\nDistribution:\nF(4,2055)\n\n\nAvg Obs:\n20.000\n\n\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nconst\n1.1318\n0.1150\n9.8430\n0.0000\n0.9063\n1.3573\n\n\nseasonal_control\n0.2284\n0.0034\n66.389\n0.0000\n0.2216\n0.2351\n\n\ntrend\n0.2850\n0.0043\n66.675\n0.0000\n0.2766\n0.2934\n\n\ncovariate_1\n-0.0165\n0.0095\n-1.7267\n0.0844\n-0.0352\n0.0022\n\n\ncovariate_2\n0.0764\n0.0096\n8.0036\n0.0000\n0.0577\n0.0952\n\n\n\n\n\n\nsales_demo_data.attrs['betas']\n\narray([-0.01237172,  0.07431322])\n\n\n\nfitted_model_standard.variance_decomposition\n\nEffects                   0.264310\nResidual                  0.010396\nPercent due to Effects    0.962157\nName: Variance Decomposition, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Sales model fitted with sub-normed dependent variable\n\n\n\n\n\n\n\n\n\n\n\nModel Summary\n\n\n\n\n\n\nfitted_model_sub.summary\n\n\nRandomEffects Estimation Summary\n\n\nDep. Variable:\nsales\nR-squared:\n0.7740\n\n\nEstimator:\nRandomEffects\nR-squared (Between):\n-1.327e+28\n\n\nNo. Observations:\n2060\nR-squared (Within):\n0.7754\n\n\nDate:\nWed, May 21 2025\nR-squared (Overall):\n0.7740\n\n\nTime:\n21:57:24\nLog-likelihood\n1785.6\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n1759.1\n\n\nEntities:\n20\nP-value\n0.0000\n\n\nAvg Obs:\n103.00\nDistribution:\nF(4,2055)\n\n\nMin Obs:\n103.00\n\n\n\n\nMax Obs:\n103.00\nF-statistic (robust):\n1759.1\n\n\n\n\nP-value\n0.0000\n\n\nTime periods:\n103\nDistribution:\nF(4,2055)\n\n\nAvg Obs:\n20.000\n\n\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nconst\n-0.2942\n0.0048\n-60.719\n0.0000\n-0.3037\n-0.2847\n\n\nseasonal_control\n0.2284\n0.0034\n66.429\n0.0000\n0.2216\n0.2351\n\n\ntrend\n0.2850\n0.0043\n66.710\n0.0000\n0.2766\n0.2934\n\n\ncovariate_1\n-0.0140\n0.0091\n-1.5283\n0.1266\n-0.0319\n0.0040\n\n\ncovariate_2\n0.0729\n0.0092\n7.9594\n0.0000\n0.0550\n0.0909\n\n\n\n\n\n\nsales_demo_data.attrs['betas']\n\narray([-0.01237172,  0.07431322])\n\n\n\nfitted_model_sub.variance_decomposition\n\nEffects                   0.000000\nResidual                  0.010396\nPercent due to Effects    0.000000\nName: Variance Decomposition, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: The divided MEM model and the standard MEM model produce much different prediction. Using the wrong model structure will produce biased predictions, and missleading results.\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{reda,\n  author = {Reda, Matthew},\n  title = {Centering in {Panel} {Models}},\n  url = {https://redam94.github.io/common_regression_issues/normalization_in_panel_models.html},\n  langid = {en},\n  abstract = {Panel regression models analyze data from the same units\n    observed over multiple time periods, enabling control for\n    time-invariant unobserved unit characteristics and estimation of\n    within-unit effects. A common technique, particularly in\n    fixed-effects models, involves “demeaning” – subtracting\n    unit-specific means from both dependent and independent variables to\n    isolate within-unit variation. While powerful, this and related\n    centering approaches have pitfalls: they preclude estimation of\n    time-invariant predictor effects, and coefficients reflect purely\n    within-unit changes, which may differ from between-unit or overall\n    effects. Such methods focus analysis on within-unit variation,\n    potentially overlooking broader patterns if this variation is\n    minimal, and can complicate the interpretation of interaction terms.\n    Centering techniques can also be missapplied, for example by only\n    being applied to the dependent variable or by dividing by instead of\n    subtracting the group level mean. Consequently, the choice of\n    centering technique fundamentally shapes the questions being\n    addressed and the interpretation of results.}\n}\nFor attribution, please cite this work as:\nReda, Matthew. n.d. “Centering in Panel Models.” https://redam94.github.io/common_regression_issues/normalization_in_panel_models.html.",
    "crumbs": [
      "Centering in Panel Models"
    ]
  },
  {
    "objectID": "synthetic_data/multicollinearity_example_data.html",
    "href": "synthetic_data/multicollinearity_example_data.html",
    "title": "Data Generation for Demonstrating Multicollinearity",
    "section": "",
    "text": "Paid Media on Sales\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nOLV Impression\n\nOLV Impression\n\n\n\nSeasonality-&gt;OLV Impression\n\n\n\n\n\nDemand\n\nDemand\n\n\n\nSeasonality-&gt;Demand\n\n\n\n\n\nSocial Impression\n\nSocial Impression\n\n\n\nSeasonality-&gt;Social Impression\n\n\n\n\n\nSearch Query\n\nSearch Query\n\n\n\nSeasonality-&gt;Search Query\n\n\n\n\n\nOLV Impression-&gt;Demand\n\n\n\n\n\nVideo Platform Sentiment\n\nVideo Platform Sentiment\n\n\n\nVideo Platform Sentiment-&gt;OLV Impression\n\n\n\n\n\nDemand-&gt;Search Query\n\n\n\n\n\nPaid Search Click\n\nPaid Search Click\n\n\n\nDemand-&gt;Paid Search Click\n\n\n\n\n\nSales\n\nSales\n\n\n\nDemand-&gt;Sales\n\n\n\n\n\nSocial Impression-&gt;Demand\n\n\n\n\n\nAuction\n\nAuction\n\n\n\nSearch Query-&gt;Auction\n\n\n\n\n\nPaid Search Impression\n\nPaid Search Impression\n\n\n\nSearch Query-&gt;Paid Search Impression\n\n\n\n\n\nSearch Query-&gt;Paid Search Click\n\n\n\n\n\nOrganic Search\n\nOrganic Search\n\n\n\nSearch Query-&gt;Organic Search\n\n\n\n\n\nAuction-&gt;Paid Search Impression\n\n\n\n\n\nPaid Search Impression-&gt;Paid Search Click\n\n\n\n\n\nPaid Search Click-&gt;Sales\n\n\n\n\n\nOrganic Search-&gt;Sales\n\n\n\n\n\nPrice\n\nPrice\n\n\n\nPrice-&gt;Demand\n\n\n\n\n\nPrice-&gt;Sales\n\n\n\n\n\n\n\n\nFigure 1: Causal Model of the data generating process\n\n\n\n\n\n\nsource\n\nhill\n\n hill (x, K=1, n=1.2)\n\nHill tranformation\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\ninput array\n\n\nK\nint\n1\nHalf saturation point\n\n\nn\nfloat\n1.2\nShape parameter\n\n\n\n\nsource\n\n\nsample_random_data\n\n sample_random_data (N_weeks:int, include_hidden_confounds:bool=False,\n                     random_seed:int|None=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN_weeks\nint\n\nNumber of weeks to generate\n\n\ninclude_hidden_confounds\nbool\nFalse\nShould hidden confounds be included in the dataset\n\n\nrandom_seed\nint | None\nNone\nRandom Seed\n\n\nReturns\nDataset\n\nDataset containing the variables described by the above causal model\n\n\n\n\ndataset = sample_random_data(156, random_seed=2, include_hidden_confounds=True)\ndataset.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 520B\nDimensions:                  (Period: 5)\nCoordinates:\n  * Period                   (Period) datetime64[ns] 40B 2021-01-04 ... 2021-...\nData variables:\n    price                    (Period) float64 40B 3.856 3.767 3.767 4.3 4.3\n    season                   (Period) float64 40B -0.9613 -0.9732 ... -1.0\n    olv_sentiment            (Period) int64 40B 0 0 0 0 0\n    social_impressions       (Period) float64 40B 4.516e+03 ... 2.289e+03\n    olv_impressions          (Period) float64 40B 1.69e+04 ... 1.227e+04\n    demand                   (Period) float64 40B 28.92 29.4 31.64 27.92 26.14\n    search_query             (Period) float64 40B 2.182e+06 ... 2.366e+06\n    auction                  (Period) float64 40B 0.07852 0.08937 ... 0.0854\n    paid_search_impressions  (Period) float64 40B 1.713e+05 ... 2.021e+05\n    paid_search_clicks       (Period) float64 40B 669.2 938.3 ... 645.8 636.1\n    organic_search           (Period) float64 40B 2.054e+06 ... 2.055e+06\n    sales                    (Period) float64 40B 6.47e+03 ... 6.29e+03\nAttributes:\n    olv_params:     {'K': 0.7960875579897572, 'n': 1.0072716356157956}\n    social_params:  {'K': 0.9767776434467591, 'n': 2.7744792328982424}\n    olv_beta:       0.21671224557085256\n    social_beta:    0.06881513644667057xarray.DatasetDimensions:Period: 5Coordinates: (1)Period(Period)datetime64[ns]2021-01-04 ... 2021-02-01array(['2021-01-04T00:00:00.000000000', '2021-01-11T00:00:00.000000000',\n       '2021-01-18T00:00:00.000000000', '2021-01-25T00:00:00.000000000',\n       '2021-02-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (12)price(Period)float643.856 3.767 3.767 4.3 4.3array([3.85623064, 3.76666366, 3.76666366, 4.30004006, 4.30004006])season(Period)float64-0.9613 -0.9732 -0.986 -0.9962 -1.0array([-0.96125225, -0.97320297, -0.9859537 , -0.99622428, -1.        ])olv_sentiment(Period)int640 0 0 0 0array([0, 0, 0, 0, 0])social_impressions(Period)float644.516e+03 2.69e+03 ... 2.289e+03array([4516.32985667, 2690.15487369, 4408.82594089,  821.07433792,\n       2289.26431861])olv_impressions(Period)float641.69e+04 1.443e+04 ... 1.227e+04array([16902.29928553, 14428.15003436, 10031.36337111, 12919.47391971,\n       12268.58976982])demand(Period)float6428.92 29.4 31.64 27.92 26.14array([28.91586164, 29.39881976, 31.63871304, 27.92203104, 26.14024436])search_query(Period)float642.182e+06 3.005e+06 ... 2.366e+06array([2182051.27979312, 3005187.1995009 , 2901400.41364904,\n       1465383.57215318, 2366366.85440288])auction(Period)float640.07852 0.08937 ... 0.1123 0.0854array([0.07852333, 0.08937114, 0.08316361, 0.11232354, 0.08539509])paid_search_impressions(Period)float641.713e+05 2.686e+05 ... 2.021e+05array([171341.93333533, 268577.00396097, 241290.93840434, 164597.06925048,\n       202076.10225716])paid_search_clicks(Period)float64669.2 938.3 1.133e+03 645.8 636.1array([ 669.21744438,  938.33665502, 1132.50226126,  645.80771116,\n        636.05519104])organic_search(Period)float642.054e+06 2.688e+06 ... 2.055e+06array([2053759.98996706, 2688297.54792605, 2580925.8707826 ,\n       1345931.37962197, 2054808.91711597])sales(Period)float646.47e+03 6.702e+03 ... 6.29e+03array([6470.24406348, 6702.13210895, 9193.50249468, 6272.93497578,\n       6290.24525121])Indexes: (1)PeriodPandasIndexPandasIndex(DatetimeIndex(['2021-01-04', '2021-01-11', '2021-01-18', '2021-01-25',\n               '2021-02-01'],\n              dtype='datetime64[ns]', name='Period', freq='W-MON'))Attributes: (4)olv_params :{'K': 0.7960875579897572, 'n': 1.0072716356157956}social_params :{'K': 0.9767776434467591, 'n': 2.7744792328982424}olv_beta :0.21671224557085256social_beta :0.06881513644667057",
    "crumbs": [
      "synthetic_data",
      "Data Generation for Demonstrating Multicollinearity"
    ]
  },
  {
    "objectID": "synthetic_data/grouped_data.html",
    "href": "synthetic_data/grouped_data.html",
    "title": "Grouped Regression Data",
    "section": "",
    "text": "source\n\ngenerate_grouped_data\n\n generate_grouped_data (sample_size:int, n_exogenous_vars:int,\n                        n_confounders:int=0, n_groups:int=1,\n                        n_group_attributes:int=0, group_var:float=3,\n                        noise_sigma:float=1,\n                        random_effect_assumption_satified:bool=True,\n                        random_seed:Optional[int]=None)\n\nGenerate grouped regression data\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsample_size\nint\n\nNumber of samples per group\n\n\nn_exogenous_vars\nint\n\nNumber of exogenous variables\n\n\nn_confounders\nint\n0\nNumber of confounder variables\n\n\nn_groups\nint\n1\nNumber of independent groups\n\n\nn_group_attributes\nint\n0\nNumber of group level attributes\n\n\ngroup_var\nfloat\n3\nVariance between groups\n\n\nnoise_sigma\nfloat\n1\nStd of noise for each observation\n\n\nrandom_effect_assumption_satified\nbool\nTrue\nIs random effect assumption valid\n\n\nrandom_seed\nOptional\nNone\n\n\n\nReturns\nDataset\n\nSynthetic dataset\n\n\n\n\ndata = generate_grouped_data(\n    100, 3, \n    n_confounders=1, n_groups=5, \n    random_seed=42, \n    random_effect_assumption_satified=False)\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1kB\nDimensions:  (group: 5, index: 5)\nCoordinates:\n  * group    (group) &lt;U7 140B 'group_0' 'group_1' 'group_2' 'group_3' 'group_4'\n  * index    (index) int64 40B 0 1 2 3 4\nData variables:\n    var_0    (group, index) float64 200B -4.29 -5.247 -3.251 ... 2.224 2.565\n    var_1    (group, index) float64 200B -6.002 -6.595 -4.969 ... 3.201 2.787\n    var_2    (group, index) float64 200B -6.836 -5.554 -4.949 ... 0.9918 1.975\n    con_0    (group, index) float64 200B 0.3047 -1.04 0.7505 ... -0.3937 0.5212\n    depvar   (group, index) float64 200B -20.39 -19.84 -17.33 ... 6.005 7.054\nAttributes:\n    true_alpha:  [[ -9.12693519]\\n [-17.69415699]\\n [ -3.57316472]\\n [ 14.463...\n    true_betas:  {'var_0': -0.0102, 'var_1': 1.5902, 'var_2': 0.1593}xarray.DatasetDimensions:group: 5index: 5Coordinates: (2)group(group)&lt;U7'group_0' 'group_1' ... 'group_4'array(['group_0', 'group_1', 'group_2', 'group_3', 'group_4'], dtype='&lt;U7')index(index)int640 1 2 3 4array([0, 1, 2, 3, 4])Data variables: (5)var_0(group, index)float64-4.29 -5.247 -3.251 ... 2.224 2.565array([[ -4.28993336,  -5.24701375,  -3.25126756,  -2.66829812,  -7.96874031],\n       [ -8.87000995,  -7.83520097, -10.5691001 ,  -7.08088071,  -9.83225184],\n       [ -1.49435767,  -0.23375686,  -3.03502379,  -2.62791234,  -4.05308145],\n       [  9.05369034,   4.48173864,   8.6913756 ,   6.35172464,   7.36224679],\n       [  0.45741539,   2.58032894,   3.60114386,   2.22384135,   2.56476205]])var_1(group, index)float64-6.002 -6.595 ... 3.201 2.787array([[-6.00174201, -6.59544669, -4.96867735, -3.7252721 , -6.63981273],\n       [-9.37953311, -7.53548951, -9.04050391, -7.00667713, -9.99760142],\n       [-2.35404051, -3.3344299 , -3.23959722, -3.17685254, -6.02774525],\n       [ 9.15203236,  6.39415109,  9.81129207,  6.70474757,  7.5229281 ],\n       [ 2.42470019,  2.86860677,  1.20691427,  3.20135074,  2.78657969]])var_2(group, index)float64-6.836 -5.554 ... 0.9918 1.975array([[ -6.83612943,  -5.5535033 ,  -4.94906834,  -6.01101911,  -1.90366831],\n       [ -8.44989128, -10.41341108,  -8.01211621,  -6.4169537 ,  -9.90583764],\n       [ -2.29872482,  -3.96801742,  -1.88125553,  -2.98965978,  -0.99604602],\n       [  5.95186917,   8.47554027,   6.11814611,   8.10250144,   8.17854086],\n       [ -1.46338471,   2.3732775 ,  -2.31343828,   0.99178827,   1.9749268 ]])con_0(group, index)float640.3047 -1.04 ... -0.3937 0.5212array([[ 0.30471708, -1.03998411,  0.7504512 ,  0.94056472, -1.95103519],\n       [-0.37816255,  1.2992283 , -0.35626397,  0.73751557, -0.93361768],\n       [ 0.33757455,  1.40748186,  0.09058491,  0.64393879, -2.0501721 ],\n       [ 1.72735021, -1.5338614 ,  0.86382801, -0.32852522, -0.06132435],\n       [-0.17961141,  0.1967761 ,  0.82052848, -0.39374117,  0.52116726]])depvar(group, index)float64-20.39 -19.84 ... 6.005 7.054array([[-20.38511533, -19.83524377, -17.32791668, -15.9930025 , -20.6428855 ],\n       [-33.74712208, -31.077567  , -32.45654011, -29.25188177, -34.99982213],\n       [ -7.77510303, -10.19414017, -10.48167343,  -8.99991138, -13.98672133],\n       [ 30.22903947,  28.21302496,  30.65645951,  26.77967958,  28.91338538],\n       [  5.62018538,   8.15337138,   1.79307287,   6.00522818,   7.05355403]])Indexes: (2)groupPandasIndexPandasIndex(Index(['group_0', 'group_1', 'group_2', 'group_3', 'group_4'], dtype='object', name='group'))indexPandasIndexPandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='index'))Attributes: (2)true_alpha :[[ -9.12693519]\n [-17.69415699]\n [ -3.57316472]\n [ 14.4630254 ]\n [  2.62920112]]true_betas :{'var_0': -0.0102, 'var_1': 1.5902, 'var_2': 0.1593}\n\n\n\ndf = data.to_dataframe().reset_index()\ndf_with_dummies = df.join(pd.get_dummies(df.group).astype(int))\ndf_with_dummies.head()\n\n\n\n\n\n\n\n\ngroup\nindex\nvar_0\nvar_1\nvar_2\ncon_0\ndepvar\ngroup_0\ngroup_1\ngroup_2\ngroup_3\ngroup_4\n\n\n\n\n0\ngroup_0\n0\n-4.289933\n-6.001742\n-6.836129\n0.304717\n-20.385115\n1\n0\n0\n0\n0\n\n\n1\ngroup_0\n1\n-5.247014\n-6.595447\n-5.553503\n-1.039984\n-19.835244\n1\n0\n0\n0\n0\n\n\n2\ngroup_0\n2\n-3.251268\n-4.968677\n-4.949068\n0.750451\n-17.327917\n1\n0\n0\n0\n0\n\n\n3\ngroup_0\n3\n-2.668298\n-3.725272\n-6.011019\n0.940565\n-15.993002\n1\n0\n0\n0\n0\n\n\n4\ngroup_0\n4\n-7.968740\n-6.639813\n-1.903668\n-1.951035\n-20.642885\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster_model = sm.OLS(\n    df_with_dummies['depvar'], \n    df_with_dummies[[\n        'group_0', 'group_1', \n        'group_2', 'group_3', \n        'group_4', 'var_0', \n        'var_1', 'var_2', \n        'con_0'\n        ]]\n    ).fit(\n        cov_type='cluster', \n        cov_kwds={\n            'groups': pd.factorize(df_with_dummies[\"group\"])[0]\n            }, \n        use_t=True)\ncluster_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared:\n0.998\n\n\nModel:\nOLS\nAdj. R-squared:\n0.998\n\n\nMethod:\nLeast Squares\nF-statistic:\nnan\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\nnan\n\n\nTime:\n22:13:46\nLog-Likelihood:\n-714.05\n\n\nNo. Observations:\n500\nAIC:\n1446.\n\n\nDf Residuals:\n491\nBIC:\n1484.\n\n\nDf Model:\n8\n\n\n\n\nCovariance Type:\ncluster\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ngroup_0\n-8.6226\n0.297\n-29.009\n0.000\n-9.448\n-7.797\n\n\ngroup_1\n-16.5405\n0.535\n-30.937\n0.000\n-18.025\n-15.056\n\n\ngroup_2\n-3.3568\n0.124\n-27.174\n0.000\n-3.700\n-3.014\n\n\ngroup_3\n13.6005\n0.424\n32.043\n0.000\n12.422\n14.779\n\n\ngroup_4\n2.5296\n0.066\n38.340\n0.000\n2.346\n2.713\n\n\nvar_0\n0.0602\n0.029\n2.107\n0.103\n-0.019\n0.140\n\n\nvar_1\n1.5980\n0.028\n57.342\n0.000\n1.521\n1.675\n\n\nvar_2\n0.2006\n0.063\n3.200\n0.033\n0.027\n0.375\n\n\ncon_0\n-0.1632\n0.079\n-2.075\n0.107\n-0.382\n0.055\n\n\n\n\n\n\n\n\nOmnibus:\n1.163\nDurbin-Watson:\n1.803\n\n\nProb(Omnibus):\n0.559\nJarque-Bera (JB):\n0.962\n\n\nSkew:\n-0.062\nProb(JB):\n0.618\n\n\nKurtosis:\n3.176\nCond. No.\n221.\n\n\n\nNotes:[1] Standard Errors are robust to cluster correlation (cluster)\n\n\n\nsimple_ols_model = sm.OLS(\n    df_with_dummies['depvar'], \n    sm.add_constant(df_with_dummies[['var_0', 'var_1', 'var_2', 'con_0']])\n    ).fit(use_t=True)\nsimple_ols_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared:\n0.995\n\n\nModel:\nOLS\nAdj. R-squared:\n0.995\n\n\nMethod:\nLeast Squares\nF-statistic:\n2.412e+04\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n22:13:47\nLog-Likelihood:\n-900.38\n\n\nNo. Observations:\n500\nAIC:\n1811.\n\n\nDf Residuals:\n495\nBIC:\n1832.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.1839\n0.069\n2.683\n0.008\n0.049\n0.319\n\n\nvar_0\n0.7034\n0.053\n13.397\n0.000\n0.600\n0.807\n\n\nvar_1\n2.1893\n0.052\n41.741\n0.000\n2.086\n2.292\n\n\nvar_2\n0.8212\n0.053\n15.420\n0.000\n0.717\n0.926\n\n\ncon_0\n-1.0991\n0.120\n-9.138\n0.000\n-1.335\n-0.863\n\n\n\n\n\n\n\n\nOmnibus:\n7.551\nDurbin-Watson:\n1.857\n\n\nProb(Omnibus):\n0.023\nJarque-Bera (JB):\n8.085\n\n\nSkew:\n-0.222\nProb(JB):\n0.0176\n\n\nKurtosis:\n3.438\nCond. No.\n19.9\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmean_adj = data - data.mean(dim='index')\ndf_demeaned = mean_adj.to_dataframe().reset_index()\ndf_demeaned_with_dummies = (\n    df_demeaned\n    .join(\n        pd.get_dummies(df_demeaned.group)\n    .astype(int)\n    )\n)\ndf_demeaned_with_dummies.head()\n\n\n\n\n\n\n\n\ngroup\nindex\nvar_0\nvar_1\nvar_2\ncon_0\ndepvar\ngroup_0\ngroup_1\ngroup_2\ngroup_3\ngroup_4\n\n\n\n\n0\ngroup_0\n0\n0.481425\n-1.406211\n-2.004544\n0.354987\n-3.170462\n1\n0\n0\n0\n0\n\n\n1\ngroup_0\n1\n-0.475656\n-1.999916\n-0.721918\n-0.989714\n-2.620590\n1\n0\n0\n0\n0\n\n\n2\ngroup_0\n2\n1.520091\n-0.373147\n-0.117483\n0.800721\n-0.113263\n1\n0\n0\n0\n0\n\n\n3\ngroup_0\n3\n2.103060\n0.870259\n-1.179433\n0.990834\n1.221651\n1\n0\n0\n0\n0\n\n\n4\ngroup_0\n4\n-3.197382\n-2.044282\n2.927917\n-1.900766\n-3.428232\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nsimple_ols_model_demeaned = sm.OLS(\n    df_demeaned_with_dummies['depvar'], \n    (df_demeaned_with_dummies[['var_0', 'var_1', 'var_2', 'con_0']])\n    ).fit(use_t=True)\nsimple_ols_model_demeaned.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared (uncentered):\n0.795\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.793\n\n\nMethod:\nLeast Squares\nF-statistic:\n481.0\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\n3.90e-169\n\n\nTime:\n22:13:52\nLog-Likelihood:\n-714.05\n\n\nNo. Observations:\n500\nAIC:\n1436.\n\n\nDf Residuals:\n496\nBIC:\n1453.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nvar_0\n0.0602\n0.046\n1.322\n0.187\n-0.029\n0.150\n\n\nvar_1\n1.5980\n0.044\n36.038\n0.000\n1.511\n1.685\n\n\nvar_2\n0.2006\n0.045\n4.421\n0.000\n0.111\n0.290\n\n\ncon_0\n-0.1632\n0.092\n-1.766\n0.078\n-0.345\n0.018\n\n\n\n\n\n\n\n\nOmnibus:\n1.163\nDurbin-Watson:\n1.803\n\n\nProb(Omnibus):\n0.559\nJarque-Bera (JB):\n0.962\n\n\nSkew:\n-0.062\nProb(JB):\n0.618\n\n\nKurtosis:\n3.176\nCond. No.\n4.73\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsimple_ols_model_demeaned = sm.OLS(\n    df_demeaned_with_dummies['depvar'], \n    (df_demeaned_with_dummies[['var_0', 'var_1', 'var_2', 'con_0']])\n    ).fit(\n        cov_type='cluster', \n        cov_kwds={\n            'groups': df_demeaned_with_dummies['group']\n            }, \n        use_t=True)\nsimple_ols_model_demeaned.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared (uncentered):\n0.795\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.793\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.833e+04\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\n2.04e-09\n\n\nTime:\n22:27:47\nLog-Likelihood:\n-714.05\n\n\nNo. Observations:\n500\nAIC:\n1436.\n\n\nDf Residuals:\n496\nBIC:\n1453.\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\ncluster\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nvar_0\n0.0602\n0.028\n2.118\n0.102\n-0.019\n0.139\n\n\nvar_1\n1.5980\n0.028\n57.633\n0.000\n1.521\n1.675\n\n\nvar_2\n0.2006\n0.062\n3.217\n0.032\n0.027\n0.374\n\n\ncon_0\n-0.1632\n0.078\n-2.085\n0.105\n-0.380\n0.054\n\n\n\n\n\n\n\n\nOmnibus:\n1.163\nDurbin-Watson:\n1.803\n\n\nProb(Omnibus):\n0.559\nJarque-Bera (JB):\n0.962\n\n\nSkew:\n-0.062\nProb(JB):\n0.618\n\n\nKurtosis:\n3.176\nCond. No.\n4.73\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors are robust to cluster correlation (cluster)\n\n\n\nsimple_ols_model_demeaned_depvar = sm.OLS(\n    df_demeaned_with_dummies['depvar'], \n    (df_with_dummies[\n        ['group_0', 'group_1', \n        'group_2', 'group_3', \n        'group_4','var_0', \n        'var_1', 'var_2', \n        'con_0']])\n    ).fit(\n        cov_type='cluster', \n        cov_kwds={\n            'groups': df_demeaned_with_dummies['group']\n            }, \n        use_t=True)\nsimple_ols_model_demeaned_depvar.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared:\n0.795\n\n\nModel:\nOLS\nAdj. R-squared:\n0.792\n\n\nMethod:\nLeast Squares\nF-statistic:\nnan\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\nnan\n\n\nTime:\n22:28:30\nLog-Likelihood:\n-714.05\n\n\nNo. Observations:\n500\nAIC:\n1446.\n\n\nDf Residuals:\n491\nBIC:\n1484.\n\n\nDf Model:\n8\n\n\n\n\nCovariance Type:\ncluster\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\ngroup_0\n8.5920\n0.297\n28.907\n0.000\n7.767\n9.417\n\n\ngroup_1\n16.5130\n0.535\n30.886\n0.000\n15.029\n17.997\n\n\ngroup_2\n3.9143\n0.124\n31.687\n0.000\n3.571\n4.257\n\n\ngroup_3\n-13.5953\n0.424\n-32.031\n0.000\n-14.774\n-12.417\n\n\ngroup_4\n-2.4543\n0.066\n-37.198\n0.000\n-2.637\n-2.271\n\n\nvar_0\n0.0602\n0.029\n2.107\n0.103\n-0.019\n0.140\n\n\nvar_1\n1.5980\n0.028\n57.342\n0.000\n1.521\n1.675\n\n\nvar_2\n0.2006\n0.063\n3.200\n0.033\n0.027\n0.375\n\n\ncon_0\n-0.1632\n0.079\n-2.075\n0.107\n-0.382\n0.055\n\n\n\n\n\n\n\n\nOmnibus:\n1.163\nDurbin-Watson:\n1.803\n\n\nProb(Omnibus):\n0.559\nJarque-Bera (JB):\n0.962\n\n\nSkew:\n-0.062\nProb(JB):\n0.618\n\n\nKurtosis:\n3.176\nCond. No.\n221.\n\n\n\nNotes:[1] Standard Errors are robust to cluster correlation (cluster)\n\n\n\nsmf.mixedlm(\n    'depvar ~ var_0 + var_1 + var_2 + con_0', \n    data=df_with_dummies, \n    groups=df_with_dummies['group']\n    ).fit(\n        method=[\"lbfgs\"]\n    ).summary()\n\n\n\n\nModel:\nMixedLM\nDependent Variable:\ndepvar\n\n\nNo. Observations:\n500\nMethod:\nREML\n\n\nNo. Groups:\n5\nScale:\n1.0373\n\n\nMin. group size:\n100\nLog-Likelihood:\n-745.7758\n\n\nMax. group size:\n100\nConverged:\nYes\n\n\nMean group size:\n100.0\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.458\n5.051\n-0.487\n0.627\n-12.358\n7.443\n\n\nvar_0\n0.065\n0.046\n1.419\n0.156\n-0.025\n0.155\n\n\nvar_1\n1.602\n0.045\n35.910\n0.000\n1.515\n1.690\n\n\nvar_2\n0.205\n0.046\n4.495\n0.000\n0.116\n0.295\n\n\ncon_0\n-0.170\n0.093\n-1.832\n0.067\n-0.352\n0.012\n\n\nGroup Var\n127.501\n89.028",
    "crumbs": [
      "synthetic_data",
      "Grouped Regression Data"
    ]
  },
  {
    "objectID": "sampling_error.html",
    "href": "sampling_error.html",
    "title": "Sampling Error in Exogenous Variables",
    "section": "",
    "text": "Error in exogenous variables—those not influenced by other variables in a model—can significantly compromise the integrity of regression analyses. When these variables are inaccurately measured and the errors are not properly addressed, several issues may arise:\nThis website is dedicated to exploring the challenges posed by sampling errors in exogenous variables within regression models. We provide comprehensive insights into how these errors can distort results and discuss effective strategies to mitigate their impact.\nBy understanding and addressing sampling errors in exogenous variables, data scientists can enhance the validity and reliability of their regression analyses.",
    "crumbs": [
      "Sampling Error in Exogenous Variables"
    ]
  },
  {
    "objectID": "sampling_error.html#survey-data",
    "href": "sampling_error.html#survey-data",
    "title": "Sampling Error in Exogenous Variables",
    "section": "Survey Data",
    "text": "Survey Data\nWhen analyzing survey data, it’s crucial to assess the precision of population parameter estimates. This precision is influenced by factors such as sample size, sampling design, and measurement error within the survey data.",
    "crumbs": [
      "Sampling Error in Exogenous Variables"
    ]
  },
  {
    "objectID": "sampling_error.html#impact-of-sampling-error-on-regression-models",
    "href": "sampling_error.html#impact-of-sampling-error-on-regression-models",
    "title": "Sampling Error in Exogenous Variables",
    "section": "Impact of Sampling Error on Regression Models",
    "text": "Impact of Sampling Error on Regression Models\nIncorporating imprecise measurements into regression models can lead to biased and inconsistent coefficient estimates. This issue persists even with unbiased sampling designs and accurate respondent answers. In survey research, where sample sizes are often limited and sampling designs complex, measurement errors can significantly affect the precision of population parameter estimates.\n\nExample 1 (Sampling Error in a Binary Outcome Variable) Consider a weekly survey involving approximately 500 participants, selected randomly to represent the general population. Participants are asked if they recall seeing a specific brand’s advertisement, with data collected via phone and online methods.\nAssuming perfect recall accuracy, we aim to estimate the effect of advertisement recall on the brand’s sales using a simple linear regression model. To explore this, we can simulate three years of survey data to analyze the relationship between advertisement recall and sales.\n\n\n\n\n\n\n\nHelper Functions\n\n\n\n\n\n\nsource\n\nrandom_walk_awareness_model\n\n random_walk_awareness_model\n                              (periods:list|pandas.core.indexes.datetimes.\n                              DatetimeIndex|numpy.ndarray)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nperiods\nlist | pandas.core.indexes.datetimes.DatetimeIndex | numpy.ndarray\nTime periods to simulate\n\n\nReturns\nModel\nPyMC model for the random walk awareness model\n\n\n\n\ndates = pd.date_range(start='2021-01-01', periods=156, freq='W-MON')\nawareness_model = random_walk_awareness_model(dates)\nstarting_awareness = 0.025\nlogit_starting_awareness = np.log(starting_awareness/(1-starting_awareness))\ngenerative_model = pm.do(\n  awareness_model, \n  {\n    'weekly_variation': .1, \n    'initial_awareness': logit_starting_awareness,\n    'weekly_shock': .01\n  }\n)\npopulation_awareness = pm.draw(generative_model['awareness'], random_seed=23)\npopulation_awareness = xr.DataArray(\n  population_awareness,\n  dims=['Period'],\n  coords={'Period': dates}\n)\n\n\n\n\n\n\n\n\n\nFigure 1: Population Awareness\n\n\n\n\n\n\nsource\n\n\nsurvey_obs_model\n\n survey_obs_model (population_awareness:xarray.core.dataarray.DataArray|py\n                   tensor.tensor.variable.TensorVariable,\n                   avg_weekly_participants:float=500.0, coords:dict=None,\n                   model:pymc.model.core.Model=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npopulation_awareness\nxarray.core.dataarray.DataArray | pytensor.tensor.variable.TensorVariable\n\nPopulation awareness\n\n\navg_weekly_participants\nfloat\n500.0\nAverage number of participants per week\n\n\ncoords\ndict\nNone\nCoordinates for the PyMC model\n\n\nmodel\nModel\nNone\nPyMC model to add the survey observation model\n\n\nReturns\nModel\n\n\n\n\n\n\nsource\n\n\nsimulate_awareness_survey_data\n\n simulate_awareness_survey_data (start_date:str='2020-01-01',\n                                 n_weeks:int=156,\n                                 avg_weekly_participants:float=500.0,\n                                 weekly_awareness_variation:float=0.08,\n                                 starting_population_aware:float=0.025,\n                                 weekly_shock:float=0.01,\n                                 random_seed:int=42)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart_date\nstr\n2020-01-01\nStart date of the survey data\n\n\nn_weeks\nint\n156\nNumber of weeks to simulate\n\n\navg_weekly_participants\nfloat\n500.0\nAverage number of participants per week\n\n\nweekly_awareness_variation\nfloat\n0.08\nStd. dev. of gaussian inovations for weekly awareness\n\n\nstarting_population_aware\nfloat\n0.025\nStarting population awareness\n\n\nweekly_shock\nfloat\n0.01\nStd. dev. of gaussian noise for weekly deviation from random walk\n\n\nrandom_seed\nint\n42\nRandom seed for reproducibility\n\n\nReturns\nDataset\n\nSimulated awareness survey data as an xarray dataset\n\n\n\n\nsource\n\n\nplot_survey_sim_data\n\n plot_survey_sim_data (data:xarray.core.dataset.Dataset)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nDataset\nSimulated survey data must contain ‘awareness’ and ‘estimated_awareness’ variables\n\n\nReturns\nNone\nPlot of the simulated survey data",
    "crumbs": [
      "Sampling Error in Exogenous Variables"
    ]
  },
  {
    "objectID": "sampling_error.html#simulation-approach",
    "href": "sampling_error.html#simulation-approach",
    "title": "Sampling Error in Exogenous Variables",
    "section": "Simulation Approach",
    "text": "Simulation Approach\n\nData Generation: Create a dataset representing weekly survey responses over three years, including variables for advertisement recall (binary) and corresponding sales figures.\nModel Specification: Define a linear regression model with sales as the dependent variable and advertisement recall as the independent variable.\nAnalysis: Fit the model to the simulated data to assess the estimated effect of advertisement recall on sales.\n\n\nGenerate Survey Responses\n\ntrace = simulate_awareness_survey_data(random_seed=23)\nplot_survey_sim_data(trace)\n\nSampling: [_noise, logit_awareness, n_positive, n_survey_participants]\n\n\n\n\n\n\n\n\nFigure 2: Simulated Awareness Survey Data\n\n\n\n\n\n\n\nGenerate Sales Data\nThe sales data is simulated using the following equation:\n\\[\n\\begin{align*}\nlog(S_t) &= \\beta \\text{pop\\_awareness}_t + \\alpha + \\varepsilon_t \\\\\n\\varepsilon_t &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{align*}\n\\tag{1}\\]\nLets see if the true coeff \\(\\beta\\) can be estimated using the simulated data.\n\nACTUAL_AWARENESS_COEFF = 30\nlog_sales = trace.awareness*ACTUAL_AWARENESS_COEFF + 10 + np.random.normal(0, 0.03, trace.awareness.shape)\nsales = np.exp(log_sales)\n\n\n\n\n\n\n\n\n\nFigure 3: Simulated sales data. See Equation 1 for data generative process\n\n\n\n\n\n\n\nThe naive model\nLet’s try ignoring the data generation process and fit a simple linear regression model to the data.\n\n\n\n\nTable 1: Using estimated awareness to predict sales\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nawareness\nR-squared:\n0.416\n\n\nModel:\nOLS\nAdj. R-squared:\n0.413\n\n\nMethod:\nLeast Squares\nF-statistic:\n89.24\n\n\nDate:\nFri, 17 Jan 2025\nProb (F-statistic):\n5.44e-17\n\n\nTime:\n03:12:36\nLog-Likelihood:\n131.73\n\n\nNo. Observations:\n156\nAIC:\n-259.5\n\n\nDf Residuals:\n154\nBIC:\n-253.4\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n10.2451\n0.019\n540.755\n0.000\n10.208\n10.282\n\n\nestimated_awareness\n12.5093\n1.324\n9.446\n0.000\n9.914\n15.105\n\n\n\n\n\n\n\n\nOmnibus:\n4.668\nDurbin-Watson:\n1.021\n\n\nProb(Omnibus):\n0.097\nJarque-Bera (JB):\n3.383\n\n\nSkew:\n0.219\nProb(JB):\n0.184\n\n\nKurtosis:\n2.427\nCond. No.\n142.\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 1 lags and without small sample correction\n\n\n\n\n\nWe can see from the results in Table 1 that the estimated coefficient is biased. The true coefficient for the effect of the populations ability to recall the brand’s advertisement on the brand’s sales is 30. The estimated coefficient is much less.\n\n\nNext the simple moving average model\nLet’s try a simple moving average model to see if we can improve the estimate of the coefficient. We will ignore the data generation process and take the moving average of the estimated awareness directly.\n\n\n\n\n\n\n\n\nFigure 4: Moving average awareness\n\n\n\n\n\n\n\n\n\nTable 2: Using moving average awareness as a predictor\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nawareness\nR-squared:\n0.733\n\n\nModel:\nOLS\nAdj. R-squared:\n0.731\n\n\nMethod:\nLeast Squares\nF-statistic:\n263.4\n\n\nDate:\nFri, 17 Jan 2025\nProb (F-statistic):\n7.67e-35\n\n\nTime:\n03:12:39\nLog-Likelihood:\n192.63\n\n\nNo. Observations:\n152\nAIC:\n-381.3\n\n\nDf Residuals:\n150\nBIC:\n-375.2\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n10.0966\n0.017\n580.819\n0.000\n10.063\n10.131\n\n\nestimated_awareness\n23.5746\n1.452\n16.231\n0.000\n20.728\n26.421\n\n\n\n\n\n\n\n\nOmnibus:\n1.668\nDurbin-Watson:\n0.903\n\n\nProb(Omnibus):\n0.434\nJarque-Bera (JB):\n1.307\n\n\nSkew:\n0.058\nProb(JB):\n0.520\n\n\nKurtosis:\n3.439\nCond. No.\n209.\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 1 lags and without small sample correction\n\n\n\n\n\nWe can see from Table 2 that we are doing better than the naive model. The estimated coefficient is closer to the true coefficient. However, the estimated coefficient is still biased.\n\n\nMoving Average (Correctly this time)\nLet’s try a moving average model again, but this time we will take the moving average of the number of survey participants and the number of positive results before dividing each.\n\nmoving_sum_n_positive = trace.n_positive.rolling(Period=5).sum().shift(Period=-2)\nmoving_sum_n_participants = trace.n_survey_participants.rolling(Period=5).sum().shift(Period=-2)\nmoving_avg_awareness = moving_sum_n_positive/moving_sum_n_participants\n\n\n\n\n\n\n\n\n\nFigure 5: Moving average awareness second approach\n\n\n\n\n\n\n\n\n\nTable 3: Corrected Moving Average Model\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nawareness\nR-squared:\n0.732\n\n\nModel:\nOLS\nAdj. R-squared:\n0.730\n\n\nMethod:\nLeast Squares\nF-statistic:\n166.6\n\n\nDate:\nFri, 17 Jan 2025\nProb (F-statistic):\n4.18e-26\n\n\nTime:\n03:12:43\nLog-Likelihood:\n192.21\n\n\nNo. Observations:\n152\nAIC:\n-380.4\n\n\nDf Residuals:\n150\nBIC:\n-374.4\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n10.0953\n0.022\n463.446\n0.000\n10.053\n10.138\n\n\nMoving Avg Awareness\n23.7208\n1.838\n12.907\n0.000\n20.119\n27.323\n\n\n\n\n\n\n\n\nOmnibus:\n2.137\nDurbin-Watson:\n0.897\n\n\nProb(Omnibus):\n0.344\nJarque-Bera (JB):\n1.827\n\n\nSkew:\n0.098\nProb(JB):\n0.401\n\n\nKurtosis:\n3.500\nCond. No.\n210.\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\n\n\n\nThis model (Table 3) is only slightly better than the simple moving average model. The estimated coefficient is still biased.\n\n\nLatent Variable Model\nLet us now try to first estimate the population level awareness using a bayesian model and then use the estimated population level awareness in the regression model.\n\ndates = trace[\"Period\"].values\nawareness_model = random_walk_awareness_model(dates)\n\nwith awareness_model as survey_model:\n    survey_obs_model(awareness_model['awareness'], avg_weekly_participants=500, coords={'Period': dates})\n    \nwith pm.observe(\n  pm.do(\n    survey_model, \n    {'n_survey_participants': trace.n_survey_participants.values} # apply the number of survey participants\n    ), \n  {'n_positive': trace.n_positive.values} # observe the number of positive responses\n  ):\n    obs_trace = pm.sample(nuts_sampler='nutpie', random_seed=42)\n\n\n\n\n\n\n\n\n\nFigure 6: Modeled Awareness\n\n\n\n\n\n\n\n\n\nTable 4: Using modeled awareness to estimate sales\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nawareness\nR-squared:\n0.845\n\n\nModel:\nOLS\nAdj. R-squared:\n0.844\n\n\nMethod:\nLeast Squares\nF-statistic:\n366.1\n\n\nDate:\nFri, 17 Jan 2025\nProb (F-statistic):\n1.51e-42\n\n\nTime:\n03:12:57\nLog-Likelihood:\n235.25\n\n\nNo. Observations:\n156\nAIC:\n-466.5\n\n\nDf Residuals:\n154\nBIC:\n-460.4\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n10.0118\n0.019\n533.362\n0.000\n9.975\n10.049\n\n\nawareness\n29.9679\n1.566\n19.135\n0.000\n26.898\n33.037\n\n\n\n\n\n\n\n\nOmnibus:\n2.650\nDurbin-Watson:\n1.050\n\n\nProb(Omnibus):\n0.266\nJarque-Bera (JB):\n2.187\n\n\nSkew:\n-0.261\nProb(JB):\n0.335\n\n\nKurtosis:\n3.252\nCond. No.\n239.\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\n\n\n\nCompared to the previous models the Latent Variable Model is much better at recovering the ground truth. While the estimated coefficient is still biased (we haven’t removed all the measurement error), it is much closer to the true coefficient, than the previous models. Given that the model can be train quickly on the data and the estimated coefficient is much closer to the true coefficient, using the latent model is a good choice.\n\n\nUsing the true awareness\nFinally, let’s see how well we can do if we use the true awareness in the regression model. This is not likely to be possible in practice, but it should provide a good comparison point.\n\n\n\n\nTable 5: Using true awareness to estimate sales\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nawareness\nR-squared:\n0.957\n\n\nModel:\nOLS\nAdj. R-squared:\n0.957\n\n\nMethod:\nLeast Squares\nF-statistic:\n4545.\n\n\nDate:\nFri, 17 Jan 2025\nProb (F-statistic):\n3.24e-116\n\n\nTime:\n03:13:00\nLog-Likelihood:\n335.30\n\n\nNo. Observations:\n156\nAIC:\n-666.6\n\n\nDf Residuals:\n154\nBIC:\n-660.5\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nHAC\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n9.9905\n0.007\n1465.262\n0.000\n9.977\n10.004\n\n\nawareness\n30.7167\n0.456\n67.415\n0.000\n29.824\n31.610\n\n\n\n\n\n\n\n\nOmnibus:\n0.434\nDurbin-Watson:\n2.231\n\n\nProb(Omnibus):\n0.805\nJarque-Bera (JB):\n0.372\n\n\nSkew:\n-0.119\nProb(JB):\n0.830\n\n\nKurtosis:\n2.973\nCond. No.\n231.\n\n\n\nNotes:[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 5 lags and without small sample correction\n\n\n\n\n\nHere we see that not only is the estimate spot on, but the standard error is also much lower than the other models. The better the measure of the exogenous variable, the more precise the estimate of the coefficient we can achieve.",
    "crumbs": [
      "Sampling Error in Exogenous Variables"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html",
    "href": "the_illusion_of_significance.html",
    "title": "The Illusion of Significance",
    "section": "",
    "text": "We’ve all seen it (perhaps even presented it); the perfectly constructed model with impressive-looking p-values that is used to justify adding millions in budget to a particular marketing channel. But how much can we really trust these insights? Statistical modeling approaches that rely on p-value thresholds (or \\(\\left| \\text{t-stat}\\right| &gt; 1\\)) for variable selection create a fundamental problem: the same data that builds the model also validates it, creating a circular logic that inevitably results in overconfidence. Methods like stepwise regression don’t just produce slightly inaccurate models: they systematically generate biased coefficients, invalid statistical tests, and unstable predictions. When these flawed models drive budget decisions, organizations end up misallocating resources based on what amounts to statistical mirages rather than genuine insights.\nBayesian statistics offers a more realistic alternative that acknowledges what traditional methods try to hide. Uncertainty is real! It should inform our decisions and not be artificially removed through statistical hocus-pocus for a slick deck. By treating parameters as variables with probability distributions and formally incorporating prior knowledge, Bayesian methods can help regularize models; prevent them from making wild claims the data can’t support. Techniques like spike-and-slab priors let us evaluate which variables truly matter without arbitrary cutoffs, while Bayesian Model Averaging sensibly hedges bets across multiple plausible models rather than on a single one. Though implementing Bayesian approaches does require more computational power, technical skill, and careful thought about prior assumptions, the payoff is substantial: more stable estimates, intuitive uncertainty measures that stakeholders can actually understand (seriously what is a confidence interval), and ultimately, budget allocation insights based on reality rather than statistical illusions.",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html#from-illusion-to-insight-tldr",
    "href": "the_illusion_of_significance.html#from-illusion-to-insight-tldr",
    "title": "The Illusion of Significance",
    "section": "",
    "text": "We’ve all seen it (perhaps even presented it); the perfectly constructed model with impressive-looking p-values that is used to justify adding millions in budget to a particular marketing channel. But how much can we really trust these insights? Statistical modeling approaches that rely on p-value thresholds (or \\(\\left| \\text{t-stat}\\right| &gt; 1\\)) for variable selection create a fundamental problem: the same data that builds the model also validates it, creating a circular logic that inevitably results in overconfidence. Methods like stepwise regression don’t just produce slightly inaccurate models: they systematically generate biased coefficients, invalid statistical tests, and unstable predictions. When these flawed models drive budget decisions, organizations end up misallocating resources based on what amounts to statistical mirages rather than genuine insights.\nBayesian statistics offers a more realistic alternative that acknowledges what traditional methods try to hide. Uncertainty is real! It should inform our decisions and not be artificially removed through statistical hocus-pocus for a slick deck. By treating parameters as variables with probability distributions and formally incorporating prior knowledge, Bayesian methods can help regularize models; prevent them from making wild claims the data can’t support. Techniques like spike-and-slab priors let us evaluate which variables truly matter without arbitrary cutoffs, while Bayesian Model Averaging sensibly hedges bets across multiple plausible models rather than on a single one. Though implementing Bayesian approaches does require more computational power, technical skill, and careful thought about prior assumptions, the payoff is substantial: more stable estimates, intuitive uncertainty measures that stakeholders can actually understand (seriously what is a confidence interval), and ultimately, budget allocation insights based on reality rather than statistical illusions.",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html#introduction-modeling-for-decision-making",
    "href": "the_illusion_of_significance.html#introduction-modeling-for-decision-making",
    "title": "The Illusion of Significance",
    "section": "Introduction: Modeling for Decision Making",
    "text": "Introduction: Modeling for Decision Making\n\nThe Hidden Costs of Misspecification: Why Model Selection Matters\nStatistical models, particularly within econometrics and marketing analytics, serve as critical instruments for dissecting complex business phenomena and guiding strategic decisions. Marketing Mix Modeling (MMM) stands as a prime example. The insights derived from MMM are frequently used for two primary purposes: understanding the historical contribution of different marketing levers and, crucially, optimizing future budget allocations to maximize return on investment (ROI).\nMMM’s use extends beyond prediction toward causal inference: understanding the “true”, incremental impact of altering a specific input, such as increasing social advertising spend, on the outcome variable (i.e. sales). This causal understanding is paramount for effective budget optimization.\nHowever, standard modeling procedures, particularly those involving selection based on statistical significance, prioritize in-sample fit over robustness and causal validity. This focus inadvertently introduces substantial biases and instability. When models that systematically misrepresent causal effects guide budget decisions, organizations face significant resource misallocation that directly impacts profitability. The aggregated and time-series nature of marketing data further compounds these challenges, as weekly or monthly observations limit statistical power while still requiring controls for seasonality, economic trends, and competitive actions\nAs marketing analytics continues to drive increasingly larger investment decisions, the potential damage from flawed model specifications grows proportionally. This page examines how traditional variable selection approaches might generate misleading insights and offers a more robust Bayesian framework for addressing these methodological pitfalls.\n\n\nThe P-Value Trap: Why Significance Testing Misleads Model Selection\nWhile p-values remain ubiquitous in statistical analysis, their application in model selection creates systematic distortions. The p-value measures the probability of observing data equally or more extreme than what was collected, assuming the null hypothesis holds true. Yet this metric becomes deeply problematic when used as a selection criterion.\nCritical misconceptions persist: Statistical significance doesn’t equate to practical importance. Large datasets can render trivial effects “significant,” while genuinely important relationships may fail arbitrary thresholds. When automated procedures like stepwise regression rely on p-values for variable selection, they introduce cascading failures:\n\nThe Seven Deadly Sins of P-Value Selection\n\n\n\n\n\n\nThe math behind t-stats\n\n\n\n\n\nIn the context of linear regression the test statistic for the i-th coefficient estimate is given as:\n\\[\n\\frac{\\hat{\\beta_i}}{\\frac{\\hat{\\sigma}}{\\sqrt{n\\hat{\\text{Var}}[X_i]}}\\sqrt{VIF_i}}\n\\]\nIt’s clear from the mathematical description of the test statistic that:\n\nCoefficients (\\(\\hat{\\beta_i}\\)) with larger magnitudes — all else being equal — will be more “significant”.\nModels with lower variance (\\(\\hat{\\sigma}\\)) will lead to all coefficients being more “significant”.\nLarger sample sizes (n) will increase all coefficients t-stat.\nWith increased variance in a predictor (\\(\\hat{\\sigma}\\)) — all else being equal — that predictor will be more “significant”.\nWith increased correlation of covariate \\(X_i\\) (\\(VIF_i\\)) — all else being equal — that predictor will be less “significant”.\n\n\n\n\n1. Systematized Cherry-Picking\nStepwise procedures systematically explore variable combinations until finding “significant” patterns. This automated search through analytical choices—different predictors, transformations, and specifications—guarantees finding patterns that appear meaningful but often reflect random noise. Even examining residuals to guide variable addition constitutes this problematic practice.\n2. The Multiplicity Trap\nTesting numerous variables inflates false discovery rates exponentially. When evaluating 20 potential predictors, the chance of incorrectly declaring at least one significant approaches certainty. Standard software reports unadjusted p-values, hiding the true extent of multiple testing performed during selection.\n\n\n\n\n\n\nHelper Functions\n\n\n\n\n\n\ndef random_covariance(n_vars):\n    \"\"\"Create a random covariance matrix with a given correlation.\"\"\"\n    \n    # Create a random correlation matrix\n    A = np.random.randn(n_vars, n_vars)\n    cov_matrix = np.dot(A, A.T)\n\n    return cov_matrix\n\ndef create_data_no_signal(n_samples, n_vars, correlated=True):\n    \"\"\"Create completely random data with no signal.\"\"\"\n    \n    mean = np.zeros(n_vars)\n    if correlated:\n        cov = random_covariance(n_signal_vars+n_noise_vars)\n        var = np.diag(cov)\n        # Ensure var of variables is 1\n        cov = cov / np.sqrt(var[:, None] * var[None, :])\n    else:\n        cov = np.eye(n_signal_vars+n_noise_vars)\n\n    X = np.random.multivariate_normal(\n        mean=mean, \n        cov=cov, \n        size=n_samples\n        )\n\n    y = np.random.randn(n_samples)\n\n    return pd.DataFrame(X), pd.Series(y)\n\n\n\ndef create_data_with_signal(\n    n_samples, \n    n_signal_vars, \n    n_noise_vars, \n    betas = None,\n    correlated = True,\n    noise_level=1):\n    \"\"\"Create data with a signal in the first n_signal_vars.\"\"\"\n    \n    mean = np.zeros(n_signal_vars+n_noise_vars)\n    if correlated:\n        cov = random_covariance(n_signal_vars+n_noise_vars)\n        var = np.diag(cov)\n        # Ensure var of variables is 1\n        cov = cov / np.sqrt(var[:, None] * var[None, :])\n    else:\n        cov = np.eye(n_signal_vars+n_noise_vars)\n\n    \n    X = np.random.multivariate_normal(\n        mean=mean, \n        cov=cov,\n        size=n_samples)\n    \n    if betas is None:\n        betas = np.zeros(n_signal_vars + n_noise_vars)\n        betas[:n_signal_vars] = np.random.randn(n_signal_vars)\n        \n    assert len(betas) == n_signal_vars + n_noise_vars, \"betas must have the same length as the number of variables\"\n\n    # Create a signal in the first n_signal_vars\n    y = np.dot(X, betas) + np.random.randn(n_samples) * noise_level * np.var(np.dot(X, betas))\n\n    return pd.DataFrame(X), pd.Series(y), betas\n\ndef forward_selection(\n    X: pd.DataFrame, y: pd.Series,\n    threshold_in=0.05\n):\n    \"\"\"Perform forward selection with p-values to find the best subset of features.\"\"\"\n    \n    # Initialize variables\n    n_features = X.shape[1]\n    selected_features = []\n    remaining_features = list(X.columns)\n    np.random.shuffle(remaining_features) # Shuffle features to simulate repeated runs\n    \n    # Perform forward selection with p-values\n    while remaining_features:\n        scores_with_candidates = []\n        for candidate in remaining_features:\n            features = selected_features + [candidate]\n            X_subset = X.loc[:, features]\n            model = sm.OLS(y, sm.add_constant(X_subset)).fit()\n            p_value = model.pvalues[candidate]\n            \n            scores_with_candidates.append((p_value, candidate))\n        \n        # Sort scores and select the best candidate\n        scores_with_candidates.sort()\n        best_new_score, best_candidate = scores_with_candidates[0]\n        \n        if best_new_score &lt; threshold_in:\n            selected_features.append(best_candidate)\n            remaining_features.remove(best_candidate)\n        else:\n            break\n    return selected_features\n\ndef backward_selection(\n    X: pd.DataFrame, y: pd.Series,\n    threshold_out=0.1\n):\n    \"\"\"Perform backward selection with p-values to find the best subset of features.\"\"\"\n    \n    # Initialize variables\n    selected_features = list(X.columns)\n    \n    # Perform backward selection with p-values\n    while selected_features:\n        features = selected_features\n        X_subset = X.loc[:, features]\n        model = sm.OLS(y, sm.add_constant(X_subset)).fit()\n        \n        # Get the feature with the highest p-value\n        p_values = model.pvalues[1:]  # Exclude intercept\n        max_p_value = p_values.max()\n        \n        if max_p_value &gt; threshold_out:\n            feature_to_remove = p_values.idxmax()\n            selected_features.remove(feature_to_remove)\n        else:\n            break\n    return selected_features\n\n\n\n\n\n\n\n\n\n\nSimulate Variable Selection Based on P-Values\n\n\n\n\n\n\nn_trials = 1_000\nn_samples = 100\nn_signal_vars = 3\nn_noise_vars = 20\n\nX, y = create_data_no_signal(\n    n_trials*n_samples,\n    n_signal_vars+n_noise_vars, \n    correlated=False\n)\n\nX.columns = [f\"X{i}\" for i in range(X.shape[1])]\ny.name = \"y\"\n\n\nforward_selected_vars = []\nfor trial in range(n_trials):\n    trial_df = X.loc[trial*n_samples:(trial+1)*n_samples-1, :]\n    trial_y = y.loc[trial*n_samples:(trial+1)*n_samples-1]\n    forward_selected_vars.append(\n        forward_selection(\n            trial_df, \n            trial_y, \n            threshold_in=0.05\n        )\n    )\n\n\nbackward_selected_vars = []\nfor trial in range(n_trials):\n    trial_df = X.loc[trial*n_samples:(trial+1)*n_samples-1, :]\n    trial_y = y.loc[trial*n_samples:(trial+1)*n_samples-1]\n    backward_selected_vars.append(\n        backward_selection(\n            trial_df, \n            trial_y, \n            threshold_out=0.05\n        )\n    )\n\n\n\n\n\nBackward SelectionFoward Selection\n\n\n\n\n\n\n\n\n\n\nFigure 1: Histogram of the number of selected variables in each trial using backward selection. The number of variables that are actually predictive are 0, however over 50% of the trials selected at least one variable as a statistically significant predictor.\n\n\n\n\n\n\n\nFamily-wise error rate: 69.7%\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histogram of the number of selected variables in each trial using forward selection. The number of variables that are actually predictive are 0, however over 50% of the trials selected at least one variable as a statistically significant predictor.\n\n\n\n\n\n\n\nFamily-wise error rate: 67.2%\n\n\n\n\n\n3. Chasing Ghosts in the Data\nSelection procedures optimize for patterns specific to the training sample, capturing random fluctuations rather than stable relationships. This aggressive pursuit of in-sample fit produces models that fail catastrophically on new data.\n\nBackward SelectionForward Selection\n\n\n\n\n\n\n\n\nModel Analysis\n\n\n\n\n\n\nSELECTION_METHOD = backward_selected_vars\nn_models_with_var = {\n    var: sum([var in selected for selected in SELECTION_METHOD])\n    for var in X.columns\n}\nin_sample_models = [\n    sm.OLS(\n        y.loc[i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X.loc[i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit() \n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nout_sample_model = [\n    sm.OLS(\n        y.loc[~i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X.loc[~i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit()\n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nin_sample_r2 = [model.rsquared for model in in_sample_models]\nout_sample_r2 = [model.rsquared for model in out_sample_model]\npredictions = [\n    model.predict(sm.add_constant(X[selected])) \n    for selected, model in zip(SELECTION_METHOD, in_sample_models)\n]\nout_of_sample_mse = [\n    np.mean((\n        y.loc[~trial*n_samples:(trial+1)*n_samples-1] \n        - pred[~trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\nin_samples_mse = [\n    np.mean((\n        y.loc[trial*n_samples:(trial+1)*n_samples-1] \n        - pred[trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: In sample and out of sample R2 of the models selected using backward selection. The models are each trained on 100 samples and tested on the remaining samples. This demonstrates the overfitting of the models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Analysis\n\n\n\n\n\n\nSELECTION_METHOD = forward_selected_vars\nn_models_with_var = {\n    var: sum([var in selected for selected in SELECTION_METHOD])\n    for var in X.columns\n}\nin_sample_models = [\n    sm.OLS(\n        y.loc[i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X.loc[i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit() \n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nout_sample_model = [\n    sm.OLS(\n        y.loc[~i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X.loc[~i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit()\n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nin_sample_r2 = [model.rsquared for model in in_sample_models]\nout_sample_r2 = [model.rsquared for model in out_sample_model]\npredictions = [\n    model.predict(sm.add_constant(X[selected])) \n    for selected, model in zip(SELECTION_METHOD, in_sample_models)\n]\nout_of_sample_mse = [\n    np.mean((\n        y.loc[~trial*n_samples:(trial+1)*n_samples-1] \n        - pred[~trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\nin_samples_mse = [\n    np.mean((\n        y.loc[trial*n_samples:(trial+1)*n_samples-1] \n        - pred[trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: In sample and out of sample R2 of the models selected using forward selection. The models are each trained on 100 samples and tested on the remaining samples. This demonstrates the overfitting of the models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation with Signal\n\n\n\n\n\n\nbetas = np.array([.2, .1, .3] + [0]*n_noise_vars)\n\nX_signal, y_signal, betas = create_data_with_signal(\n    n_trials*n_samples,\n    n_signal_vars, \n    n_noise_vars, \n    correlated=True,\n    betas=betas, \n    noise_level=3\n)\nX_signal.columns = [f\"X{i}\" for i in range(X_signal.shape[1])]\ny_signal.name = \"y\"\n\n\nforward_selected_signal_vars = []\nfor trial in range(n_trials):\n    trial_df = X_signal.loc[trial*n_samples:(trial+1)*n_samples-1, :]\n    trial_y = y_signal.loc[trial*n_samples:(trial+1)*n_samples-1]\n    forward_selected_signal_vars.append(\n        forward_selection(\n            trial_df, \n            trial_y, \n            threshold_in=0.05\n        )\n    )\n\n\nbackward_selected_signal_vars = []\nfor trial in range(n_trials):\n    trial_df = X_signal.loc[trial*n_samples:(trial+1)*n_samples-1, :]\n    trial_y = y_signal.loc[trial*n_samples:(trial+1)*n_samples-1]\n    backward_selected_signal_vars.append(\n        backward_selection(\n            trial_df, \n            trial_y, \n            threshold_out=0.05\n        )\n    )\n\n\n\n\n\nForward SelectionBackward Selection\n\n\n\n\n\n\n\n\nModel Analysis\n\n\n\n\n\n\nSELECTION_METHOD = forward_selected_signal_vars\n\nin_sample_models = [\n    sm.OLS(\n        y_signal.loc[i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X_signal.loc[i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit() \n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nout_sample_model = [\n    sm.OLS(\n        y_signal.loc[~i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X_signal.loc[~i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit()\n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nin_sample_r2 = [model.rsquared for model in in_sample_models]\nout_sample_r2 = [model.rsquared for model in out_sample_model]\npredictions = [\n    model.predict(sm.add_constant(X_signal[selected])) \n    for selected, model in zip(SELECTION_METHOD, in_sample_models)\n]\nout_of_sample_mse = [\n    np.mean((\n        y_signal.loc[~trial*n_samples:(trial+1)*n_samples-1] \n        - pred[~trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\nin_samples_mse = [\n    np.mean((\n        y_signal.loc[trial*n_samples:(trial+1)*n_samples-1] \n        - pred[trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: In sample and out of sample R2 of the models selected using forward selection. The models are each trained on 100 samples and tested on the remaining samples. This demonstrates the overfitting of the models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Analysis\n\n\n\n\n\n\nSELECTION_METHOD = backward_selected_signal_vars\n\nin_sample_models = [\n    sm.OLS(\n        y_signal.loc[i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X_signal.loc[i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit() \n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nout_sample_model = [\n    sm.OLS(\n        y_signal.loc[~i*n_samples:(i+1)*n_samples-1], \n        sm.add_constant(X_signal.loc[~i*n_samples:(i+1)*n_samples-1, selected])\n        ).fit()\n        for i, selected in enumerate(SELECTION_METHOD)\n    ]\nin_sample_r2 = [model.rsquared for model in in_sample_models]\nout_sample_r2 = [model.rsquared for model in out_sample_model]\npredictions = [\n    model.predict(sm.add_constant(X_signal[selected])) \n    for selected, model in zip(SELECTION_METHOD, in_sample_models)\n]\nout_of_sample_mse = [\n    np.mean((\n        y_signal.loc[~trial*n_samples:(trial+1)*n_samples-1] \n        - pred[~trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\nin_samples_mse = [\n    np.mean((\n        y_signal.loc[trial*n_samples:(trial+1)*n_samples-1] \n        - pred[trial*n_samples:(trial+1)*n_samples-1]\n        )**2) \n    for trial, pred in enumerate(predictions)\n]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: In sample and out of sample R2 of the models selected using backward selection. The models are each trained on 100 samples and tested on the remaining samples. This demonstrates the overfitting of the models.\n\n\n\n\n\n\n\n\n4. Systematic Bias in Effect Sizes\nSelected variables exhibit inflated coefficients—not because they’re truly important, but because the selection process favors variables that randomly showed stronger associations. This “winner’s curse” systematically overstates the magnitude of selected effects.\n\n\n\n\n\n\n\n\nFigure 7: The variable selection method systematically biases the coefficient estimates. This happens because models where the variable has a higher estimated effect are more likely to have that variable selected to be in the model. This is a form of selection bias.\n\n\n\n\n\n5. False Precision\nPost-selection standard errors understate true uncertainty. Confidence intervals appear misleadingly narrow, and p-values seem artificially compelling. These metrics assume the model was pre-specified, not constructed through data exploration, rendering them fundamentally invalid.\n\n\nCoverage of 95% confidence interval for X0: 85.3%\nCoverage of 95% confidence interval for X1: 80.1%\nCoverage of 95% confidence interval for X2: 83.1%\n\n\n6. Fragile Foundations\nMinor data perturbations can dramatically alter selected variables, especially with correlated predictors common in marketing data. Models that shift drastically with small changes provide unreliable foundations for business decisions.\n\n\nSelection frequency of model Y ~ X0 + X1 + X2: 17.8%\nSelection frequency of model Y ~ X0 + X2: 12.2%\nSelection frequency of model Y ~ X0 + X2 + X18: 1.4%\nSelection frequency of model Y ~ X0 + X2 + X22: 1.1%\nSelection frequency of model Y ~ X0 + X2 + X5: 1.1%\nSelection frequency of model Y ~ X0 + X1 + X2 + X21: 0.9%\nSelection frequency of model Y ~ X0 + X2 + X15: 0.9%\nSelection frequency of model Y ~ X0 + X1 + X2 + X15: 0.8%\nSelection frequency of model Y ~ X0 + X1 + X2 + X19: 0.8%\nSelection frequency of model Y ~ X0 + X2 + X4: 0.7%\nSelection frequency of other models: 62.3%\n\n\n7. Transformation Theater\nSelecting functional forms (logarithmic, polynomial, adstock curves) based on significance compounds these problems. Testing hundreds of possible transformations virtually guarantees finding spurious patterns that optimize noise rather than signal.\nThe fundamental flaw: Using the same data for both model construction and inference violates core statistical principles. When data drives model specification, subsequent p-values and confidence intervals lose their theoretical justification. The model becomes a product of exploration, not a hypothesis to test.\n\n\n\nWhen Statistical Illusions Drive Business Decisions\nThe fundamental flaws in p-value driven selection cascade into real-world consequences that directly undermine business objectives. These aren’t merely theoretical concerns—they systematically distort our understanding of marketing effectiveness and lead to predictably poor outcomes.\nFalse Signals Masquerading as Insights\nTime-series data creates fertile ground for statistical mirages. When ice cream sales and drowning incidents both spike in summer, naive selection procedures eagerly identify “significant” relationships without recognizing the underlying seasonal driver. In MMM contexts, these spurious associations proliferate: variables that merely share trends or seasonal patterns get misidentified as meaningful predictors. Each false inclusion not only clutters the model but actively degrades our ability to estimate genuine marketing effects with precision.\nThe Hidden Cost of Arbitrary Thresholds\nSignificance cutoffs create a binary world where p = 0.049 means inclusion and p = 0.051 means exclusion—despite these values being functionally identical. This arbitrary boundary systematically excludes genuine causal factors that happen to fall just above the threshold in a particular sample. When these excluded variables correlate with included predictors, their omission biases every remaining coefficient. The model doesn’t just miss one relationship; it distorts all others.\nCausal Blindness in Automated Selection\nStatistical algorithms operate without causal awareness, unable to distinguish between fundamentally different types of relationships:\n\nConfounders that must be included to block spurious associations\nMediators whose inclusion obscures the total effects we seek to measure\n\nColliders that create artificial associations when mistakenly conditioned upon\n\nProcedures that select variables based solely on statistical association routinely include variables that poison causal interpretation (mediators, colliders) while excluding those essential for valid inference (confounders). The resulting model becomes a causal minefield where coefficients represent unknown mixtures of direct effects, indirect pathways, and induced biases.\nThe Analyst’s Dilemma: When Wrong Becomes Right\nPerhaps most insidiously, these flawed models create organizational inertia that perpetuates their errors. Analysts face immense pressure to ensure their updated models produce the same high-level conclusions as previous efforts—that TV drives 30% of sales, that digital ROI is 2.5x, that seasonality accounts for 15% of variance. This leads to a troubling pattern of behaviors:\n\nReverse Engineering Results: When new data suggests different attribution patterns, analysts manipulate model specifications until familiar conclusions emerge—adjusting adstock parameters, saturation curves, or control variables not for statistical validity but to reproduce expected ROI rankings\nStrategic Time Splitting: Models that attribute “too much” or “too little” to certain channels get subdivided into multiple time periods, fracturing the data until each segment yields the “correct” share of voice\nSelective Variable Inclusion: External factors get added or removed based on how they shift attribution between channels, using statistical significance as cover for what amounts to results manipulation\nTransformation Shopping: Analysts cycle through combinations of functional forms until the model produces ROI estimates and attribution percentages that align with organizational beliefs\n\nThis creates a vicious cycle where each generation of models must reproduce not just any results, but the specific business conclusions that stakeholders have internalized. “TV should drive 25-35% of sales” becomes an unquestionable truth that models must confirm rather than test.\nFrom Flawed Models to Failed Strategies\nThese statistical distortions translate directly into business failures. Biased coefficients paint a fantasy landscape of marketing effectiveness—some channels appear artificially powerful while others seem worthless or even harmful. Budget optimization based on these illusions inevitably fails: money flows to channels whose effectiveness was overstated while truly profitable opportunities starve for investment. The false confidence provided by inappropriately narrow confidence intervals compounds the damage, encouraging aggressive reallocation based on estimates that are both wrong and wrongly certain.\nThe Reproducibility Crisis in Practice\nModels built on sample-specific quirks fail to generalize. When next quarter’s data arrives, the carefully selected variable set often produces wildly different results—or the selection procedure itself chooses entirely different variables. Rather than acknowledging this instability as evidence of flawed methodology, analysts scramble to manipulate the new model until it reproduces familiar attribution patterns and ROI hierarchies. This stability theater destroys organizational learning: teams spend more effort confirming past conclusions than discovering new truths. Strategic planning becomes impossible when the “insights” guiding it are actually fossilized artifacts of historical data mining.\nThe compound effect is devastating: flawed statistical procedures produce unstable models with biased coefficients, which generate incorrect ROI estimates that become organizational “truth,” leading to poor budget allocation and ultimately reduced business performance. What begins as a methodological shortcut evolves into an institutional pathology where being consistently wrong becomes preferable to admitting uncertainty.\n\n\nThe Reality Gap: When Exploratory Analysis Masquerades as Causal Insight\nModern Marketing Mix Modeling exists in a troubling state of methodological confusion. Practitioners routinely present their models as predictive engines capable of forecasting future performance, while stakeholders interpret and apply these same models as causal instruments for optimizing budget allocation. The uncomfortable truth is that most MMM implementations are, at best, sophisticated exploratory analyses dressed up in the language of prediction and causation.\nThis disconnect between claims and capabilities creates a cascade of poor decisions. When exploratory models—built through p-value mining and post-hoc selection—are treated as causal truth, organizations systematically misallocate resources based on statistical mirages. The biased coefficients and overfitted relationships that emerge from flawed selection procedures don’t merely represent academic concerns; they translate directly into wasted marketing spend and missed opportunities.\nPerhaps most perniciously, these flawed models become the benchmarks against which new methodologies are evaluated. When a novel approach produces different results from traditional stepwise regression, it’s often dismissed as “inconsistent with established models” rather than recognized as potentially more accurate. This creates a self-reinforcing cycle where statistical illusions become institutionalized truths, and genuine improvements are rejected for failing to reproduce familiar biases.\nThe path forward requires honest acknowledgment of what current MMM practices actually deliver: exploratory insights that might suggest interesting patterns but cannot reliably predict future outcomes or identify causal relationships. Until the field adopts more rigorous methodologies—whether through Bayesian frameworks, proper regularization, or genuine causal inference techniques—we must stop pretending that models built on circular logic and data dredging can guide multi-million dollar decisions. The stakes are too high, and the methods too flawed, to continue conflating exploration with explanation.",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html#the-path-forward-getting-comfortable-with-uncertainty",
    "href": "the_illusion_of_significance.html#the-path-forward-getting-comfortable-with-uncertainty",
    "title": "The Illusion of Significance",
    "section": "The Path Forward: Getting Comfortable with Uncertainty",
    "text": "The Path Forward: Getting Comfortable with Uncertainty\n\nThe Uncertainty Laundering Machine\nStatistician Andrew Gelman offers a devastating critique of how statistics is abused that perfectly captures the MMM crisis:\n\n“The ethics comes in if we think of this entire journal publication system as a sort of machine for laundering uncertainty: researchers start with junk data (for example, poorly-thought-out experiments on college students, or surveys of online Mechanical Turk participants) and then work with the data, straining out the null results and reporting what is statistically significant, in a process analogous to the notorious mortgage lenders of the mid-2000s, who created high-value ‘tranches’ out of subprime loans. The loan crisis precipitated an economic recession, and I doubt the replication crisis will trigger such a crash in science. But I see a crucial similarity in that technical methods (structured finance for mortgages; statistical significance for scientific research) were being used to create value out of thin air.”\n\nThis analogy illuminates exactly what’s happening in MMM. We start with inherently uncertain, observational data—weekly sales figures confounded by countless factors, media spend that varies with business strategy, competitive actions we can’t fully observe. Then we run this “junk data” through our statistical machinery, straining out the non-significant variables, p-hacking our way to impressive results, and packaging the survivors as “insights.”\nJust as subprime mortgages got repackaged as AAA-rated securities, uncertain correlations get transformed into “proven” ROI figures. The technical sophistication of the process—the complex adstock transformations, the multi-stage regression models, the impressive p-values—serves the same function as the financial engineering of the 2000s: it launders uncertainty, creating an appearance of value and reliability that doesn’t actually exist.\n\n\nThe MMM Value Mirage\nIn marketing analytics, this laundering process follows a predictable pattern:\n\nStart with messy reality: Aggregated time series with ~200 observations, dozens of correlated variables, unobserved confounders\nApply the machinery: Stepwise regression, transformation selection, significance testing\nStrain out uncertainty: Discard “non-significant” variables, ignore wide confidence intervals, suppress unstable results\nPackage as certainty: “TV delivers 3.2x ROI (p &lt; 0.001)”\n\nThe organization receives what appears to be valuable intelligence—precise ROI estimates, clear attribution percentages, “data-driven” recommendations. But like those mortgage-backed securities, the value is illusory. We’ve used technical methods to create certainty out of thin air.\n\n\nFrom False Precision to Honest Ignorance\nThe marketing analytics industry has become addicted not just to certainty, but to this process of manufacturing it. Stakeholders demand precise ROI figures, and analysts have built an entire infrastructure—not to find truth, but to launder the fundamental uncertainty of marketing effectiveness into seemingly solid insights. The narrow confidence intervals and impressive p-values aren’t evidence of precision; they’re the output of a machine designed to hide uncertainty, not quantify it.\nThe first step forward is dismantling the laundering machine. We must stop using statistical sophistication to obscure uncertainty and start using it for its proper purpose: to honestly quantify and communicate what we don’t know. Real marketing effects are messy, context-dependent, and riddled with uncertainty. Any methodology that claims otherwise is selling false comfort rather than actionable insight.\n\n\nWhy Uncertainty Is Your Friend, Not Your Enemy\nUncertainty isn’t a flaw to be hidden—it’s crucial information that should guide decision-making. Consider two scenarios:\n\nTraditional approach: “TV ROI is 3.2x”\nHonest approach: “TV ROI is likely between 1.5x and 5x, with our best estimate at 3x”\n\nThe second statement contains more useful information. It acknowledges that TV might be marginally profitable or highly lucrative, suggesting a portfolio approach rather than an all-or-nothing strategy. The traditional approach’s false precision encourages overconfident bets based on point estimates that are likely wrong.\nUncertainty quantification enables better decisions:\n\nPortfolio thinking: Wide credible intervals suggest diversification\nExperimentation priorities: High uncertainty indicates where tests would be most valuable\nRisk management: Understanding the range of possible outcomes prevents catastrophic misallocation\nStakeholder trust: Acknowledging limitations builds more credibility than false precision\n\n\n\nBayesian Methods: Where Uncertainty Is a Feature, Not a Bug\nBayesian approaches offer a natural framework for embracing uncertainty. Rather than producing single “best” models with spurious precision, Bayesian methods:\n\nQuantify parameter uncertainty: Every coefficient comes with a full probability distribution, not just a point estimate\nPropagate uncertainty honestly: Uncertainty in parameters flows through to uncertainty in predictions and decisions\nIncorporate prior knowledge: Historical information and business constraints enter through priors, not through p-hacking\nAverage over possibilities: Instead of selecting one model, Bayesian Model Averaging acknowledges multiple plausible realities\n\nWhen implemented properly, these methods produce results that might initially disappoint those seeking false precision: “Digital marketing ROI is probably positive (85% probability above 1.0x) with a median estimate of 1.8x and substantial uncertainty (90% credible interval: 0.7x to 4.2x).” Yet this honest assessment provides far more value than a precisely wrong point estimate.\n\n\nPractical Steps Toward Uncertainty-Aware MMM\n1. Reframe the Conversation\n\nStop asking “What’s the exact ROI?” and start asking “What range of ROIs is plausible?”\nReplace “Which model is correct?” with “What do multiple reasonable models suggest?”\nShift from “Prove this channel works” to “How confident are we about this channel’s effectiveness?”\n\n2. Adopt Robust Methodologies\n\nImplement regularization techniques that acknowledge coefficient uncertainty\nUse Bayesian methods with thoughtful priors based on accumulated knowledge\nEmploy ensemble approaches that combine multiple models rather than selecting one\nValidate through hold-out testing that honestly assesses predictive uncertainty\n\n3. Visualize Uncertainty\n\nReplace tables of point estimates with distributional plots\nShow credible/confidence intervals prominently, not as footnotes\nUse scenario planning based on uncertainty ranges\nCreate decision rules that explicitly account for uncertainty levels\n\n4. Build Organizational Comfort\n\nEducate stakeholders that uncertainty is information, not incompetence\nCelebrate decisions that acknowledge and plan for uncertainty\nReward analysts who surface uncomfortable uncertainties over those who provide false comfort\nCreate processes that use uncertainty to guide experimentation priorities\n\n\n\nThe Competitive Advantage of Honest Uncertainty\nOrganizations that embrace uncertainty gain a paradoxical advantage: by admitting what they don’t know, they make better decisions than competitors clinging to false precision. They:\n\nAvoid catastrophic bets based on spuriously precise estimates\nIdentify genuine opportunities that uncertainty-averse competitors miss\nBuild resilient strategies that perform well across plausible scenarios\nLearn faster by focusing experimentation where uncertainty is highest\nAdapt more quickly when reality diverges from estimates\n\n\n\nA New Standard for MMM Excellence\nThe future of marketing analytics lies not in ever-more-sophisticated methods for manufacturing certainty, but in frameworks that honestly quantify and communicate what we don’t know. Excellence should be measured not by how narrow our confidence intervals are, but by how well our uncertainty estimates reflect reality.\nThis requires a fundamental shift: from viewing uncertainty as a problem to solve, to recognizing it as essential information for decision-making. Only by getting comfortable with uncertainty can we move beyond the statistical theater that currently dominates MMM and toward methods that genuinely improve marketing effectiveness.\nThe path forward is clear: Embrace uncertainty, quantify it honestly, and use it to make more robust decisions. The alternative—continuing to pretend we know more than we do—has already proven its capacity for expensive failure.",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html#from-statistical-theater-to-decision-science-a-synthetic-case-study",
    "href": "the_illusion_of_significance.html#from-statistical-theater-to-decision-science-a-synthetic-case-study",
    "title": "The Illusion of Significance",
    "section": "From Statistical Theater to Decision Science: A Synthetic Case Study",
    "text": "From Statistical Theater to Decision Science: A Synthetic Case Study\n\nIntroduction: Two Paradigms, Two Futures\nTo illustrate the stark contrast between current practices and what’s possible, we present a synthetic case study of a hypothetical consumer brand allocating $50 million across marketing channels. We’ll analyze the same dataset through two fundamentally different lenses:\nThe Status Quo: A traditional frequentist approach that relies on stepwise selection, p-value thresholds, and point estimates—the statistical laundering machine in action.\nThe Alternative: A Bayesian causal framework that explicitly models uncertainty, incorporates domain knowledge, and acknowledges multiple plausible realities—an honest attempt to support decision-making under uncertainty.\nThis isn’t merely an academic exercise. The differences in these approaches translate directly into millions of dollars in marketing effectiveness. One path leads to overconfident bets based on statistical mirages; the other to robust strategies that perform well across the range of plausible truths. The following analysis demonstrates not just why change is necessary, but what that change looks like in practice.\n\n\n1. The Business Context and Data Generating Process\n\nCompany background and marketing challenge\nTrue underlying causal structure (hidden from analysts)\nSimulated data generation with realistic confounders\nObservable variables vs. latent factors\n\n\n\n\n\n\n\nSynthetic Data Generation\n\n\n\n\n\n\ndef hill_transformation(x, alpha=1, k=0.5):\n    \"\"\"\n    Hill saturation transformation\n    x: media spend (scaled 0-1)\n    alpha: shape parameter (&lt;=1 for c-shaped curve, &gt;1 for s-shaped)\n    k: half-saturation point\n    \"\"\"\n    return x**alpha / (x**alpha + k**alpha)\n\ndef model_simulate(\n    X: pd.DataFrame,\n    coeffs_main: pd.Series,\n    intercept: float,\n    alphas: pd.Series,\n    k_values: pd.Series,\n    media_maxes: pd.Series,\n    noise_level: float = 0.1,\n    random_seed: int = 42,\n):\n    np.random.seed(random_seed)\n    X = X.copy()\n    media_vars = list(alphas.index)\n    X_scaled = X[media_vars] / (media_maxes + 1e-10)\n    \n    for media_var in media_vars:\n        X[media_var] = hill_transformation(X_scaled[media_var], alpha=alphas[media_var], k=k_values[media_var])\n    \n    contribution = X*coeffs_main\n    y_mean = X.dot(coeffs_main) + intercept\n\n    y_sample = y_mean + np.random.normal(0, noise_level, size=len(y_mean))\n\n    return X, y_mean, contribution, y_sample\n\ndef generate_synthetic_mmm_data(\n    n_weeks=156, \n    n_media=5, \n    random_seed=42\n    ):\n    \"\"\"\n    Generate realistic MMM data with complex causal structure including mediator, confounders and colliders\n    n_weeks: 3 years of weekly data\n    \"\"\"\n    np.random.seed(random_seed)\n    # Time index\n    t = np.arange(n_weeks)\n    \n    # ========================================\n    # CONFOUNDERS (affect both media spend and sales)\n    # ========================================\n    \n    # 1. Seasonality - drives both promotional activity and natural sales patterns\n    seasonality = 0.3 * np.sin(2 * np.pi * t / 52) + 0.1 * np.sin(4 * np.pi * t / 52)\n    \n    # 2. Macroeconomic conditions - affects marketing budgets and consumer spending\n    gdp_growth = 0.02 + 0.01 * np.sin(2 * np.pi * t / 104) + np.random.normal(0, 1, n_weeks).cumsum() / n_weeks\n    unemployment = 5.0 + 2.0 * np.sin(2 * np.pi * t / 156 + 1.5) + np.random.normal(0, 0.2, n_weeks)\n    consumer_confidence = 100 + 10 * gdp_growth * 10 - 2 * unemployment + np.random.normal(0, 3, n_weeks)\n    \n    # 3. Major retail events - drive both promotional spend and sales\n    holidays = np.zeros(n_weeks)\n    black_friday = [(w % 52 == 47) for w in range(n_weeks)]\n    christmas = [(w % 52 &gt;= 50) or (w % 52 &lt;= 1) for w in range(n_weeks)]\n    holidays[black_friday] = 1.5\n    holidays[christmas] = 1.0\n    \n    # 4. Competitor promotional intensity - triggers reactive spending and affects sales\n    competitor_promos = 0.3 + 0.2 * np.sin(2 * np.pi * t / 13) + 0.1 * np.random.randn(n_weeks).cumsum() / n_weeks\n    competitor_promos = np.clip(competitor_promos, 0, 1)\n    \n    # 5. Product lifecycle stage - affects marketing strategy and natural demand\n    product_age = t / 52  # Years since launch\n    product_lifecycle = np.exp(-product_age / 2) * 0.5 + 0.5  # Decaying newness effect\n    \n    # ========================================\n    # SALES DRIVERS (affect sales but NOT media spend)\n    # ========================================\n    \n    # 1. Distribution gains - more stores carrying product\n    distribution_points = 1000 + 10 * t + np.random.normal(0, 20, n_weeks).cumsum()\n    distribution_index = distribution_points / distribution_points[0]\n    \n    # 2. Product quality improvements (e.g., ratings)\n    quality_score = 4.0 + 0.02 * t / n_weeks + 0.1 * np.random.randn(n_weeks).cumsum() / n_weeks\n    quality_score = np.clip(quality_score, 3.5, 5.0)\n    \n    # 3. Word of mouth momentum (builds over time, not marketing driven)\n    wom_momentum = 0.1 * (1 - np.exp(-t / 26)) + 0.05 * np.random.randn(n_weeks).cumsum() / n_weeks\n    wom_momentum = np.clip(wom_momentum, 0, 0.3)\n    \n    # ========================================\n    # RED HERRINGS (correlated but don't affect sales)\n    # ========================================\n    \n    # 1. Weather patterns (correlate with seasonality but don't drive sales)\n    temperature = 60 + 25 * np.sin(2 * np.pi * t / 52) + np.random.normal(0, 7, n_weeks)\n    precipitation = 2 + 1.5 * np.sin(2 * np.pi * t / 52 + 3) + np.abs(np.random.normal(0, 0.5, n_weeks))\n    \n    # 2. Stock market index (correlates with economy but doesn't directly drive sales)\n    stock_index = 10000 + 50 * t + 500 * gdp_growth.cumsum() + 200 * np.random.randn(n_weeks).cumsum()\n    \n    # 3. Social media followers (grows over time but doesn't drive sales)\n    social_followers = 10000 * (1 + 0.02 * t + 0.1 * np.random.randn(n_weeks).cumsum() / n_weeks)\n    social_followers = np.maximum(social_followers, 10000)\n    \n    # 4. Website traffic (correlates with media but doesn't independently drive sales)\n    base_traffic = 50000 + 500 * t\n    \n    # 5. Number of SKUs (correlates with time but doesn't affect aggregate sales)\n    num_skus = 10 + np.floor(t / 26) + np.random.binomial(2, 0.1, n_weeks).cumsum()\n    \n    # ========================================\n    # MEDIA SPEND GENERATION\n    # ========================================\n    \n    media_names = ['TV', 'Digital', 'Social', 'Radio', 'Print']\n    media_spend_raw = np.zeros((n_weeks, n_media))\n    \n    # Budget influenced by confounders\n    total_budget_index = (1.0 + \n                         0.2 * consumer_confidence / 100 + \n                         0.6 * holidays + \n                         0.4 * product_lifecycle - \n                         0.15 * competitor_promos)\n    \n    # TV: Traditional, holiday-focused, reactive to competition\n    media_spend_raw[:, 0] = (200000 * (total_budget_index *\n                            (0.4 + 0.3 * holidays + 0.2 * seasonality - 0.1 * competitor_promos)) *\n                            (1 + 0.2 * np.random.randn(n_weeks)))\n    \n    # Digital: Growing trend, responsive to GDP\n    media_spend_raw[:, 1] = (150000 * (total_budget_index *\n                            (0.3 + 0.05 * t / n_weeks + 0.2 * gdp_growth * 10)) *\n                            (1 + 0.3 * np.random.randn(n_weeks)))\n    \n    # Social: Volatile, product-launch focused\n    media_spend_raw[:, 2] = (80000 * (total_budget_index * \n                            (0.2 + 0.4 * product_lifecycle - 0.2 * holidays)) *\n                            (1 + 0.5 * np.random.randn(n_weeks)))\n    \n    # Radio: Steady with seasonal pattern\n    media_spend_raw[:, 3] = (50000 * (total_budget_index *\n                            0.3 + 0.2 * seasonality) *\n                            (1 + 0.4 * np.random.randn(n_weeks)))\n    \n    # Print: Declining trend, older demographic\n    media_spend_raw[:, 4] = (40000 * total_budget_index * \n                            (0.4 - 0.1 * t / n_weeks) *\n                            (1 + 0.3 * np.random.randn(n_weeks)))\n    \n    # Ensure non-negative\n    media_spend_raw = np.maximum(media_spend_raw, 0)\n    \n    # Scale and transform\n    media_spend_scaled = media_spend_raw / (media_spend_raw.max(axis=0) + 1e-10)\n    media_transformed = np.zeros_like(media_spend_scaled)\n    \n    # Different saturation parameters by channel\n    alphas = [1.0, 2.0, 1.4, 1.0, 1.2]\n    k_values = [0.4, 0.3, 0.35, 0.5, 0.6]\n    \n    for i in range(n_media):\n        media_transformed[:, i] = hill_transformation(media_spend_scaled[:, i], alphas[i], k_values[i])\n    \n    # Update website traffic based on digital spend (red herring - correlates but doesn't cause sales)\n    website_traffic = base_traffic + 1000 * media_transformed[:, 1] + 500 * media_transformed[:, 2]\n    \n    # ========================================\n    # MEDIATOR VARIABLE (Media → Purchase Intent → Sales)\n    # ========================================\n    \n    # Purchase intent is driven by media spend (especially TV and Digital)\n    # This represents the percentage of consumers intending to purchase\n    purchase_intent_base = 0.15  # 15% base intent\n\n    indirect_media_effects = np.array([0.12, 0.10, 0.08, 0.04, 0.02])  # Indirect media effects on intent\n    # Media drives purchase intent\n    purchase_intent = (purchase_intent_base +\n                      0.12 * media_transformed[:, 0] +    # TV has strong effect on intent\n                      0.10 * media_transformed[:, 1] +    # Digital also drives intent\n                      0.08 * media_transformed[:, 2] +    # Social moderate effect\n                      0.04 * media_transformed[:, 3] +    # Radio small effect\n                      0.02 * media_transformed[:, 4] +    # Print minimal effect\n                      0.05 * seasonality +                # Some seasonal variation\n                      0.03 * np.random.randn(n_weeks))   # Random noise\n    \n    purchase_intent = np.clip(purchase_intent, 0, 0.6)  # Cap at 60% intent\n    \n    # ========================================\n    # TRUE SALES GENERATION (Multiplicative Model)\n    # ========================================\n    \n    \n    \n    # Direct media effects (excluding path through purchase intent)\n    # These are smaller because some effect goes through the mediator\n    direct_media_effects = np.array([0.08, 0.08, 0.04, 0.04, 0.02])\n\n    \n    # Effect of purchase intent on sales\n    purchase_intent_effect = 2.0  # Strong effect of intent on sales\n\n    # True TOTAL media effects (including path through purchase intent)\n    true_total_media_effects = indirect_media_effects * purchase_intent_effect + direct_media_effects \n\n    # Base sales (log scale)\n    log_base_sales = 6.0  # exp(6) ~ 403 base sales\n    \n    # Build sales from true drivers only\n    log_sales = (log_base_sales + \n                # Confounders\n                0.25 * seasonality +\n                0.15 * (consumer_confidence - 100) / 20 +\n                0.35 * holidays +\n                -0.20 * (competitor_promos - 0.5) +\n                0.10 * product_lifecycle +\n                # Sales-only drivers  \n                0.30 * np.log(distribution_index) +\n                0.20 * (quality_score - 4.0) +\n                0.15 * wom_momentum +\n                # MEDIATOR effect\n                purchase_intent_effect * (purchase_intent - purchase_intent_base))\n    \n    # Add DIRECT media effects only\n    for i in range(n_media):\n        log_sales += direct_media_effects[i] * media_transformed[:, i]\n    \n    # Add noise\n    log_sales += np.random.normal(0, 0.02, n_weeks)\n    \n    # Convert to sales\n    sales = np.exp(log_sales)\n    \n    # ========================================\n    # COLLIDER (affected by both media and sales)\n    # ========================================\n    \n    # Brand awareness surveys - caused by both advertising and sales success\n    brand_awareness = (20 +  # Base awareness\n                      15 * (media_transformed[:, 0] + 0.5 * media_transformed[:, 1]) +  # TV and Digital drive awareness\n                      10 * (sales / sales.mean() - 1) +  # Success drives awareness\n                      5 * np.random.randn(n_weeks))  # Noise\n    brand_awareness = np.clip(brand_awareness, 0, 100)\n    \n    # ========================================\n    # CREATE DATAFRAME\n    # ========================================\n    \n    data = pd.DataFrame({\n        'week': t,\n        'sales': sales,\n        'log_sales': log_sales,\n        \n        # Confounders\n        'seasonality': seasonality,\n        'gdp_growth': gdp_growth,\n        'unemployment': unemployment,\n        'consumer_confidence': consumer_confidence,\n        'holidays': holidays,\n        'competitor_promos': competitor_promos,\n        'product_lifecycle': product_lifecycle,\n        \n        # Sales drivers (not media drivers)\n        'distribution_index': distribution_index,\n        'quality_score': quality_score,\n        'word_of_mouth': wom_momentum,\n        \n        # Red herrings (don't affect sales)\n        'temperature': temperature,\n        'precipitation': precipitation,\n        'stock_index': stock_index,\n        'social_followers': social_followers,\n        'website_traffic': website_traffic,\n        'num_skus': num_skus,\n        \n        # MEDIATOR\n        'purchase_intent': purchase_intent,\n        \n        # Collider\n        'brand_awareness': brand_awareness\n    })\n    \n    # Add media variables\n    for i, name in enumerate(media_names):\n        data[f'{name}_spend'] = media_spend_raw[:, i]\n        data[f'{name}_spend_scaled'] = media_spend_scaled[:, i]\n        data[f'{name}_transformed'] = media_transformed[:, i]\n    \n    # Create some derived features that stepwise might pick up\n    data['temp_squared'] = data['temperature'] ** 2\n    data['awareness_change'] = data['brand_awareness'].diff().fillna(0)\n    data['intent_change'] = data['purchase_intent'].diff().fillna(0)\n    data['spend_total'] = media_spend_raw.sum(axis=1)\n    \n    return data, true_total_media_effects, direct_media_effects, {\"indirect\": indirect_media_effects,\"alphas\": alphas, \"k_values\": k_values} , media_names\n\n\n\n\n\n\n\n\n\n\nTrue Market Structure\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarket Structure\n\n\n\nSeasonality\n\nSeasonality\n\n\n\nPaid Media\n\nPaid Media\n\n\n\nSeasonality-&gt;Paid Media\n\n\n\n\n\nSales\n\nSales\n\n\n\nSeasonality-&gt;Sales\n\n\n\n\n\nTempurature\n\nTempurature\n\n\n\nSeasonality-&gt;Tempurature\n\n\n\n\n\nPercepitation\n\nPercepitation\n\n\n\nSeasonality-&gt;Percepitation\n\n\n\n\n\nProduct Lifecycle\n\nProduct Lifecycle\n\n\n\nProduct Lifecycle-&gt;Paid Media\n\n\n\n\n\nProduct Lifecycle-&gt;Sales\n\n\n\n\n\nWebsite Visits\n\nWebsite Visits\n\n\n\nPaid Media-&gt;Website Visits\n\n\n\n\n\nPurchase Intent\n\nPurchase Intent\n\n\n\nPaid Media-&gt;Purchase Intent\n\n\n\n\n\nBrand Awareness\n\nBrand Awareness\n\n\n\nPaid Media-&gt;Brand Awareness\n\n\n\n\n\nGDP Growth\n\nGDP Growth\n\n\n\nGDP Growth-&gt;Paid Media\n\n\n\n\n\nGDP Growth-&gt;Sales\n\n\n\n\n\nUnemployment\n\nUnemployment\n\n\n\nUnemployment-&gt;Paid Media\n\n\n\n\n\nUnemployment-&gt;Sales\n\n\n\n\n\nConsumer Confidence\n\nConsumer Confidence\n\n\n\nConsumer Confidence-&gt;Paid Media\n\n\n\n\n\nConsumer Confidence-&gt;Sales\n\n\n\n\n\nHolidays\n\nHolidays\n\n\n\nHolidays-&gt;Paid Media\n\n\n\n\n\nHolidays-&gt;Sales\n\n\n\n\n\nCompetitor Promotions\n\nCompetitor Promotions\n\n\n\nCompetitor Promotions-&gt;Paid Media\n\n\n\n\n\nCompetitor Promotions-&gt;Sales\n\n\n\n\n\nSales-&gt;Brand Awareness\n\n\n\n\n\nDistribution Index\n\nDistribution Index\n\n\n\nDistribution Index-&gt;Sales\n\n\n\n\n\nQuality Score\n\nQuality Score\n\n\n\nQuality Score-&gt;Sales\n\n\n\n\n\nWord of Mouth\n\nWord of Mouth\n\n\n\nWord of Mouth-&gt;Sales\n\n\n\n\n\nPurchase Intent-&gt;Sales\n\n\n\n\n\n\n\n\nFigure 8: This graph shows the actual data generating model. We typically do not have access to the full model and often we are missing data or have imperfect measures of the data.\n\n\n\n\n\n\n# Generate the data\ndata, true_total_effects, true_direct_effects, params, media_names = generate_synthetic_mmm_data(random_seed=12, n_weeks=362)\n\nprint(f\"\\nTrue TOTAL media effects (including indirect path): {dict(zip(media_names, true_total_effects))}\")\nprint(f\"True DIRECT media effects (excluding mediator path): {dict(zip(media_names, true_direct_effects))}\")\n\n\nTrue TOTAL media effects (including indirect path): {'TV': np.float64(0.32), 'Digital': np.float64(0.28), 'Social': np.float64(0.2), 'Radio': np.float64(0.12), 'Print': np.float64(0.06)}\nTrue DIRECT media effects (excluding mediator path): {'TV': np.float64(0.08), 'Digital': np.float64(0.08), 'Social': np.float64(0.04), 'Radio': np.float64(0.04), 'Print': np.float64(0.02)}\n\n\n\nplt.plot(data['TV_spend'].sort_values(), cont['TV_spend'].sort_values()*100, label='TV Spend Contribution', color='C0', alpha=0.6)\nplt.plot(data['Digital_spend'].sort_values(), cont['Digital_spend'].sort_values()*100, label='Digital Spend Contribution', color='C1', alpha=0.6)\nplt.plot(data['Social_spend'].sort_values(), cont['Social_spend'].sort_values()*100, label='Social Spend Contribution', color='C2', alpha=0.6)\nplt.plot(data['Print_spend'].sort_values(), cont['Print_spend'].sort_values()*100, label='Print Spend Contribution', color='C3', alpha=0.6)\nplt.plot(data['Radio_spend'].sort_values(), cont['Radio_spend'].sort_values()*100, label='Radio Spend Contribution', color='C4', alpha=0.6)\nplt.xlabel(\"Spend\")\nplt.ylabel(\"% Lift to Sales\")\nplt.title(\"Media Spend Contribution to Sales\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n\n\n\nGDPUnemploymentCCISales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigitalPrintSocialTVRadio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Approach A: The Frequentist P-Value Theater\n\n2.1 The Analyst’s Ritual\n\nVariable selection via stepwise regression\nTransformation shopping for media curves\nChasing significance across model specifications\n\n\n\n\n\n\n\nBase Variable Selection Code\n\n\n\n\n\n\ndef prepare_features_for_selection(data, media_names):\n    \"\"\"\n    Prepare all possible features for stepwise selection\n    \"\"\"\n    features = pd.DataFrame()\n    media_features = pd.DataFrame()\n    \n    # Media variables (transformed) (assuming transformation is already known)\n    # for name in media_names:\n    #     features[f'{name}'] = data[f'{name}_transformed']\n    \n    # All potential control variables (mixing good and bad)\n    control_vars = [\n        #'seasonality', # True Seasonality is often un-observed\n        'gdp_growth', 'unemployment', 'consumer_confidence',\n        'holidays', 'competitor_promos', 'product_lifecycle',\n        'distribution_index', 'quality_score', 'word_of_mouth',\n        'temperature', 'precipitation', 'stock_index',\n        'social_followers', 'website_traffic', 'num_skus',\n        'purchase_intent',  # The MEDIATOR!\n        'brand_awareness',  # The collider!\n        'temp_squared', 'awareness_change', 'intent_change' # 'spend_total'\n    ]\n    \n    for var in control_vars:\n        if var in data.columns:\n            features[var] = data[var]\n    \n    # # Add some interaction terms (fishing expedition)\n    # features['TV_x_holidays'] = data['TV_transformed'] * data['holidays']\n    # features['Digital_x_confidence'] = data['Digital_transformed'] * data['consumer_confidence'] / 100\n    # features['Social_x_lifecycle'] = data['Social_transformed'] * data['product_lifecycle']\n    # features['TV_x_intent'] = data['TV_transformed'] * data['purchase_intent']  # Interaction with mediator!\n    for media in media_names:\n        media_features[f\"{media}_transformed\"] = data[f'{media}_transformed']\n    for media, half_sat, n in product(media_names, np.linspace(0.3, 0.8, 6), np.linspace(1.0, 2.0, 11)):\n        media_features[f'{media}_{half_sat:0.2f}_{n:0.2f}'] = hill_transformation(data[f'{media}_spend_scaled'], n, half_sat)\n        # add lagged variables for more complexity\n        if media in media_names[:2]:\n            media_features[f'{media}_{half_sat:0.2f}_{n:0.2f}_lag1'] = media_features[f'{media}_{half_sat:0.2f}_{n:0.2f}'].shift(1).fillna(0)\n    \n    # # Add lagged variables (more complexity)\n    # for name in media_names[:2]:  # Just TV and Digital to avoid too many variables\n    #     features[f'{name}_lag1'] = data[f'{name}_transformed'].shift(1).fillna(0)\n    \n    # Time trends\n    features['trend'] = np.arange(len(data)) / len(data)\n    features['trend_sq'] = features['trend'] ** 2\n    features['log_distribution_index'] = np.log(features['distribution_index'])\n    return features, media_features\n\ndef stepwise_selection(X, y, threshold_in=0.05, threshold_out=0.10, verbose=True):\n    \"\"\"\n    Forward-backward stepwise selection based on p-values\n    \"\"\"\n    included = []\n    excluded = list(X.columns)\n    \n    best_aic = np.inf\n    \n    if verbose:\n        print(\"\\nStarting stepwise selection...\")\n        print(f\"Total candidate variables: {len(excluded)}\")\n        print(\"-\" * 80)\n    \n    step = 0\n    while True:\n        changed = False\n        \n        # Forward step\n        excluded_copy = excluded.copy()\n        best_pvalue = threshold_in + 1\n        best_feature = None\n        \n        for feature in excluded_copy:\n            features = included + [feature]\n            X_subset = X[features]\n            X_with_const = sm.add_constant(X_subset, has_constant='add')\n            \n            try:\n                model = sm.OLS(y, X_with_const).fit()\n                p_value = model.pvalues[feature]\n                \n                if p_value &lt; best_pvalue:\n                    best_pvalue = p_value\n                    best_feature = feature\n                    best_aic_candidate = model.aic\n            except:\n                continue\n        \n        if best_feature and best_pvalue &lt; threshold_in:\n            included.append(best_feature)\n            excluded.remove(best_feature)\n            changed = True\n            step += 1\n            best_aic = best_aic_candidate\n            \n            if verbose:\n                print(f\"Step {step:3d} - ADDED '{best_feature:25s}' (p={best_pvalue:.4f}, AIC={best_aic:.1f})\")\n        \n        # Backward step\n        if len(included) &gt; 0:\n            X_subset = X[included]\n            X_with_const = sm.add_constant(X_subset, has_constant='add')\n            model = sm.OLS(y, X_with_const).fit()\n            \n            # Find worst p-value\n            pvalues = model.pvalues[1:]  # Exclude intercept\n            worst_pvalue = pvalues.max()\n            worst_feature = pvalues.idxmax()\n            \n            if worst_pvalue &gt; threshold_out:\n                included.remove(worst_feature)\n                excluded.append(worst_feature)\n                changed = True\n                step += 1\n                \n                # Refit without removed variable\n                X_subset_new = X[included]\n                X_with_const_new = sm.add_constant(X_subset_new, has_constant='add')\n                model_new = sm.OLS(y, X_with_const_new).fit()\n                best_aic = model_new.aic\n                \n                if verbose:\n                    print(f\"Step {step:3d} - REMOVED '{worst_feature:25s}' (p={worst_pvalue:.4f}, AIC={best_aic:.1f})\")\n        \n        if not changed:\n            break\n    \n    if verbose:\n        print(\"-\" * 80)\n        print(f\"Final model includes {len(included)} variables\")\n\n    return included\n\n\n\n\n\n\n\n\n\n\nCode to Select Media Transform\n\n\n\n\n\n\ndef select_best_transformed_media_features(features, media_features, y, media_names, base_features):\n    \"\"\"\n    Select best transformed media features based on p-values\n    \"\"\"\n    selected_media = []\n    total_features = features.copy()\n    total_features[media_features.columns] = media_features\n    \n    for media_name in media_names:\n        best_tvalue = -np.inf\n        best_transform = \"\"\n        for transform in media_features.columns:\n            if not media_name in transform:\n                continue\n            X_subset = total_features[[transform] + base_features]\n            X_with_const = sm.add_constant(X_subset, has_constant='add')\n\n            try:\n                model = sm.OLS(y, X_with_const).fit()\n                t_value = model.tvalues[transform]\n                if t_value &gt; best_tvalue:\n                    best_tvalue = t_value\n                    best_transform = transform\n            except:\n                if best_transform == \"\":\n                    best_transform = transform\n                continue\n        \n        selected_media.append(best_transform)\n\n    \n    \n    return selected_media\n\n\n\n\nSelect Base Variables\n\n\n\nTable 1: Base model selected features using stepwise selection. The model looks reasonable, but includes some variables that will produce biased estimates.\n\n\n\n\nStarting stepwise selection...\nTotal candidate variables: 23\n--------------------------------------------------------------------------------\nStep   1 - ADDED 'holidays                 ' (p=0.0000, AIC=-386.9)\nStep   2 - ADDED 'purchase_intent          ' (p=0.0000, AIC=-692.8)\nStep   3 - ADDED 'stock_index              ' (p=0.0000, AIC=-904.5)\nStep   4 - ADDED 'precipitation            ' (p=0.0000, AIC=-1048.8)\nStep   5 - ADDED 'competitor_promos        ' (p=0.0000, AIC=-1185.2)\nStep   6 - ADDED 'consumer_confidence      ' (p=0.0000, AIC=-1334.3)\nStep   7 - ADDED 'log_distribution_index   ' (p=0.0000, AIC=-1499.1)\nStep   8 - REMOVED 'stock_index              ' (p=0.4308, AIC=-1500.5)\nStep   9 - ADDED 'temperature              ' (p=0.0000, AIC=-1519.1)\nStep  10 - ADDED 'intent_change            ' (p=0.0158, AIC=-1523.1)\nStep  11 - ADDED 'brand_awareness          ' (p=0.0164, AIC=-1527.0)\n--------------------------------------------------------------------------------\nFinal model includes 9 variables\n\n================================================================================\nFREQUENTIST SELECTED BASE MODEL\n================================================================================\n\n Base Model Fit: R2 = 0.980 DW = 1.501\n\nSelected Variables by Type:\n              Variable                Type  Coefficient  Std Error P-value    VIF\n              holidays          Confounder     0.355744   0.005805   &lt;0.01   1.67\n       purchase_intent            MEDIATOR     2.455619   0.053301   &lt;0.01 102.03\n         precipitation         RED HERRING    -0.027437   0.003015   &lt;0.01  22.07\n     competitor_promos          Confounder    -0.204070   0.011011   &lt;0.01   5.48\n   consumer_confidence          Confounder     0.007367   0.000338   &lt;0.01 161.88\nlog_distribution_index Derived/Interaction     0.257806   0.006050   &lt;0.01   8.27\n           temperature         RED HERRING     0.000785   0.000172   &lt;0.01  43.61\n         intent_change            MEDIATOR    -0.100636   0.037900   &lt;0.01   1.82\n       brand_awareness            COLLIDER     0.000702   0.000291   0.016  32.50\n\n\n\n\nThe base variables look reasonable there are no variables that can’t be justified even intent_change being negative could be interpreted as meaningful. Now that the base is mostly set it is time to add some media variables.\n\n\n\nTable 2: Media Selection: 1st Pass Promising Results\n\n\n\n\n Base Model Fit: R2 = 0.983 DW = 1.477\n\nSelected Variables by Type:\n              Variable                Type  Coefficient  Std Error P-value    VIF\n              holidays          Confounder     0.316224   0.008403   &lt;0.01   3.95\n       purchase_intent            MEDIATOR     2.230874   0.059442   &lt;0.01 156.94\n         precipitation         RED HERRING    -0.025306   0.002824   &lt;0.01  22.55\n     competitor_promos          Confounder    -0.197373   0.010458   &lt;0.01   5.65\n   consumer_confidence          Confounder     0.006896   0.000326   &lt;0.01 194.23\nlog_distribution_index Derived/Interaction     0.271841   0.006066   &lt;0.01  11.36\n           temperature         RED HERRING     0.000844   0.000165   &lt;0.01  48.88\n         intent_change            MEDIATOR    -0.072703   0.035560   0.042   1.89\n       brand_awareness            COLLIDER     0.000284   0.000277   0.306  34.24\n          TV_0.30_1.10               MEDIA     0.154601   0.028456   &lt;0.01  78.08\n   Digital_transformed               MEDIA     0.061313   0.010551   &lt;0.01  14.19\n      Social_0.30_1.80               MEDIA     0.012201   0.007102   0.087   8.41\n       Radio_0.30_1.00               MEDIA     0.041594   0.011881   &lt;0.01  20.55\n       Print_0.70_2.00               MEDIA     0.026433   0.013734   0.055   7.94\n\n\n\n\nRemove conusmer_confidence because of high VIF (reguardles of wether they are an important control or have a low P-value).\n\n\n\n Base Model Fit: R2 = 0.961 DW = 1.363\n\nSelected Variables by Type:\n              Variable                Type  Coefficient  Std Error P-value    VIF\n              holidays          Confounder     0.274952   0.014279   &lt;0.01   4.91\n       purchase_intent            MEDIATOR     2.271407   0.089305   &lt;0.01 134.09\n         precipitation         RED HERRING    -0.021244   0.004244   &lt;0.01  13.94\n     competitor_promos          Confounder    -0.223028   0.015643   &lt;0.01   5.70\nlog_distribution_index Derived/Interaction     0.220188   0.008361   &lt;0.01  11.08\n           temperature         RED HERRING     0.000934   0.000249   &lt;0.01  38.55\n         intent_change            MEDIATOR    -0.184374   0.053043   &lt;0.01   1.52\n       brand_awareness            COLLIDER     0.001045   0.000413   0.012  31.88\n          TV_0.40_2.00               MEDIA     0.130986   0.031692   &lt;0.01  21.52\n     Digital_0.30_1.30               MEDIA     0.161488   0.020682   &lt;0.01  22.40\n      Social_0.80_2.00               MEDIA     0.013254   0.017932   0.460   3.84\n       Radio_0.30_1.00               MEDIA     0.038889   0.017930   0.031  20.03\n       Print_0.80_2.00               MEDIA     0.060873   0.023353   &lt;0.01   6.75\n\n\nSocial’s impact is really small and the p-value is too large perhaps the effect doesn’t act independently of other Digital channels so combine them. Leave them on seperate s-curves because they might still saturate differently. Also purchase_intent VIF is also too high so we will remove that too.\n\n\n\n Base Model Fit: R2 = 0.880 DW = 1.295\n\nSelected Variables by Type:\n                                             Variable                Type  Coefficient  Std Error P-value   VIF\n                                             holidays          Confounder     0.341947   0.015059   &lt;0.01  1.65\n                                        precipitation         RED HERRING    -0.033155   0.007353   &lt;0.01 12.22\n                                    competitor_promos          Confounder    -0.256500   0.026846   &lt;0.01  5.53\n                               log_distribution_index Derived/Interaction     0.173744   0.014187   &lt;0.01 10.89\n                                          temperature         RED HERRING     0.002032   0.000428   &lt;0.01 27.28\n                                        intent_change            MEDIATOR     0.693345   0.076326   &lt;0.01  1.21\n                                      brand_awareness            COLLIDER     0.002543   0.000705   &lt;0.01 29.09\nSocial+Digital Social_0.80_1.00 + Digital_transformed Derived/Interaction     0.251795   0.020300   &lt;0.01 16.08\n                                    TV_0.30_1.70_lag1               MEDIA     0.138343   0.029999   &lt;0.01 11.68\n                                      Radio_0.30_1.00               MEDIA     0.119849   0.030732   &lt;0.01 17.83\n                                      Print_0.80_2.00               MEDIA     0.095220   0.040978   0.010  6.53\n\n\nSince the variable Social+Digital were combined we treat Social and Digital as if they both have the same coefficient estimate in the model. This is not standard statistical practice but it makes for contribution and ROI reporting easier.\n\n\n\nTable 3\n\n\n\n\n================================================================================\nMEDIA EFFECTS: TRUTH vs ESTIMATED\n================================================================================\n\nMEDIATOR (purchase_intent) included in model: False\n\nMedia Effect Comparison:\n--------------------------------------------------------------------------------\n\nTV:\n  True TOTAL effect:  0.3200\n  True DIRECT effect: 0.0800\n  Estimated effect:   0.1383 ± 0.0300\n  Comparing to TOTAL effect: Bias = -56.8%\n  95% CI: [0.0795, 0.1971]\n\nDigital:\n  True TOTAL effect:  0.2800\n  True DIRECT effect: 0.0800\n  Estimated effect:   0.2518 ± 0.0203\n  Comparing to TOTAL effect: Bias = -10.1%\n  95% CI: [0.2120, 0.2916]\n\nSocial:\n  True TOTAL effect:  0.2000\n  True DIRECT effect: 0.0400\n  Estimated effect:   0.2518 ± 0.0203\n  Comparing to TOTAL effect: Bias = +25.9%\n  95% CI: [0.2120, 0.2916]\n\nRadio:\n  True TOTAL effect:  0.1200\n  True DIRECT effect: 0.0400\n  Estimated effect:   0.1198 ± 0.0307\n  Comparing to TOTAL effect: Bias = -0.1%\n  95% CI: [0.0596, 0.1801]\n\nPrint:\n  True TOTAL effect:  0.0600\n  True DIRECT effect: 0.0200\n  Estimated effect:   0.0952 ± 0.0410\n  Comparing to TOTAL effect: Bias = +58.7%\n  95% CI: [0.0149, 0.1755]\n\n================================================================================\nPROBLEMATIC SELECTIONS\n================================================================================\n\nVariables that bias causal interpretation:\n  - precipitation (RED HERRING): coef=-0.0332, p=&lt;0.01\n    → No causal effect on sales, reduces precision\n  - temperature (RED HERRING): coef=0.0020, p=&lt;0.01\n    → No causal effect on sales, reduces precision\n  - intent_change (MEDIATOR): coef=0.6933, p=&lt;0.01\n    → Blocks indirect media effects, causing underestimation\n  - brand_awareness (COLLIDER): coef=0.0025, p=&lt;0.01\n    → Creates spurious associations between media and sales\n\nImportant sales drivers MISSED: ['distribution_index', 'quality_score', 'word_of_mouth']\n\nCONFOUNDERS MISSED media estimates will be biased: ['seasonality', 'gdp_growth', 'unemployment', 'consumer_confidence', 'product_lifecycle']\n\n\n\n\n\n\n\n\n\n\nBest Case Frequentest Model\n\n\n\n\n\n\n\n\n\nTable 4: Model under with the correct adjustment set and media transformation this is if you had perfect knowledge of the data generating process and is the ideal estimator for this data.\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nlog_sales\nR-squared:\n0.896\n\n\nModel:\nOLS\nAdj. R-squared:\n0.893\n\n\nMethod:\nLeast Squares\nF-statistic:\n251.2\n\n\nDate:\nTue, 27 May 2025\nProb (F-statistic):\n1.75e-163\n\n\nTime:\n00:09:32\nLog-Likelihood:\n478.70\n\n\nNo. Observations:\n362\nAIC:\n-931.4\n\n\nDf Residuals:\n349\nBIC:\n-880.8\n\n\nDf Model:\n12\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n6.1410\n0.128\n48.096\n0.000\n5.890\n6.392\n\n\nseasonality\n0.3489\n0.020\n17.761\n0.000\n0.310\n0.387\n\n\ngdp_growth\n-0.1560\n0.238\n-0.656\n0.512\n-0.624\n0.312\n\n\nunemployment\n0.0069\n0.003\n2.020\n0.044\n0.000\n0.014\n\n\nconsumer_confidence\n0.0067\n0.001\n5.906\n0.000\n0.004\n0.009\n\n\nholidays\n0.3246\n0.022\n14.873\n0.000\n0.282\n0.368\n\n\ncompetitor_promos\n-0.2119\n0.025\n-8.333\n0.000\n-0.262\n-0.162\n\n\nproduct_lifecycle\n-0.7336\n0.059\n-12.426\n0.000\n-0.850\n-0.618\n\n\nTV_transformed\n0.3961\n0.079\n5.024\n0.000\n0.241\n0.551\n\n\nDigital_transformed\n0.3162\n0.025\n12.508\n0.000\n0.266\n0.366\n\n\nSocial_transformed\n0.1903\n0.019\n10.044\n0.000\n0.153\n0.228\n\n\nRadio_transformed\n0.1743\n0.032\n5.471\n0.000\n0.112\n0.237\n\n\nPrint_transformed\n0.0509\n0.042\n1.200\n0.231\n-0.033\n0.134\n\n\n\n\n\n\n\n\nOmnibus:\n0.631\nDurbin-Watson:\n1.911\n\n\nProb(Omnibus):\n0.730\nJarque-Bera (JB):\n0.730\n\n\nSkew:\n-0.018\nProb(JB):\n0.694\n\n\nKurtosis:\n2.783\nCond. No.\n6.49e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.49e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n\n\n\nCurrent MethodologyIdeal Model\n\n\n\n\n\n\n\n\n\n\nFigure 9: Media transformations selected by stepwise selection. The model is not perfect, but it is a good approximation of the true data generating process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Media contributions estimated from the ideal model. This is the ideal estimator for this data.\n\n\n\n\n\n\n\n\n\n2.2 The Illusion of Precision\n\nFinal model results and “significant” findings\nNarrow confidence intervals and impressive p-values\nROI rankings and attribution claims\n\n2.3 The Recommended Strategy\n\nBudget reallocation based on point estimates\nProjected lift and business case\n\n\n\n\n3. Approach B: Bayesian Causal Reasoning\n\n3.1 Mapping the Causal Landscape\n\nEliciting stakeholder beliefs and concerns\nConstructing multiple plausible DAGs\nIdentifying testable implications\n\n3.2 Principled Variable Selection\n\nSpike-and-slab priors for uncertain relationships\nPosterior inclusion probabilities vs. p-values\nHandling confounders, mediators, and colliders\n\n3.3 Embracing Model Uncertainty\n\nBayesian Model Averaging across DAGs\nPosterior distributions of causal effects\nSensitivity analysis to untestable assumptions\n\n\n\n\n4. From Uncertainty to Decisions\n\n4.1 Decision Theory Meets Marketing\n\nUtility functions and business objectives\nExpected value vs. robust optimization\nPortfolio theory for marketing mix\n\n4.2 The Uncertainty-Aware Strategy\n\nAllocation recommendations with credible intervals\nHedging across model uncertainty\nValue of information analysis\n\n\n\n\n5. The Moment of Truth: Comparing Outcomes\n\n5.1 Revealed Reality\n\nThe true data generating process exposed\nHow each approach performed against truth\n\n5.2 Financial Impact\n\nROI achieved by each strategy\nCost of false certainty quantified\n\n5.3 Robustness Analysis\n\nPerformance across alternative scenarios\nWhen each approach fails catastrophically\n\n\n\n\n6. Lessons for Practice\n\n6.1 Why the Frequentist Approach Failed\n\nSpecific mechanisms of failure\nWarning signs that were ignored\n\n6.2 Why the Bayesian Approach (Mostly) Succeeded\n\nHow uncertainty quantification prevented disasters\nWhere even good methods struggle\n\n6.3 Implementation Roadmap\n\nPractical steps for organizations\nCommon obstacles and solutions\nBuilding institutional knowledge\n\n\n\n\n7. Beyond the Case Study: Implications for MMM\n\nThe scalability question\nComputational and expertise requirements\nThe organizational change challenge\nA new standard for marketing analytics",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html#consequences-of-flawed-selection-in-mmm",
    "href": "the_illusion_of_significance.html#consequences-of-flawed-selection-in-mmm",
    "title": "The Illusion of Significance",
    "section": "Consequences of Flawed Selection in MMM",
    "text": "Consequences of Flawed Selection in MMM\nEmploying naive selection methods and subsequent inference leads to significant practical problems:\n\nMisattribution of ROI & Effects: Biased estimates for marketing coefficients (ROI) and potentially inaccurate representations of media dynamics (adstock, saturation).\nFlawed Budget Allocation: Suboptimal marketing investment decisions stemming from unreliable ROI figures.\nPoor Understanding of Business Drivers: Incorrect identification of baseline factors (trends, seasonality, macroeconomics) and media response patterns.\nModel Instability & Non-Reproducibility: Selected factors and transformations may vary considerably with data updates, reducing model credibility.\nOverfitting: Models capture noise specific to the historical data, resulting in poor predictive performance for forecasting or simulations.\nMisinterpretation of Control Factor Coefficients: Attributing causal effects to the coefficients of baseline or control factors (e.g., macroeconomic variables, competitor activity) included in the model. These factors are typically observational and likely confounded themselves; their coefficients primarily reflect statistical association and adjustment needed to isolate media effects, not necessarily isolated causal impacts. This misinterpretation is related to the “Table 2 fallacy,” where coefficients from a multivariable model are improperly treated as independent causal effects.",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "the_illusion_of_significance.html#recommended-approaches-and-considerations",
    "href": "the_illusion_of_significance.html#recommended-approaches-and-considerations",
    "title": "The Illusion of Significance",
    "section": "Recommended Approaches and Considerations",
    "text": "Recommended Approaches and Considerations\nAddressing these challenges requires more robust methodologies:\n\nRigorous Time-Series Handling: Explicitly model or remove seasonality (e.g., dummies, Fourier terms, decomposition); test for and address non-stationarity (e.g., differencing); incorporate theoretically sound lags for media (adstock) and potentially external variables.\nRegularization Methods (LASSO, Ridge, Elastic Net): Handle many predictors simultaneously, perform coefficient shrinkage and implicit variable selection, often yielding more stable results than stepwise methods. Must be applied in conjunction with appropriate time-series structures.\nInformation Criteria (AIC, BIC): Use for comparing non-nested models that correctly account for time-series properties, providing a more principled approach than p-value thresholds alone.\nTime-Series Cross-Validation: Employ methods like rolling-origin validation to assess out-of-sample predictive performance robustly.\nBayesian Frameworks: Offer a probabilistic approach to uncertainty.\n\nPriors on Functional Forms: Incorporate prior knowledge or average over plausible media transformations (adstock/saturation) instead of hard selection.\nSparsity-Inducing Priors (e.g., Regularized Horseshoe): Provide principled variable selection for external factors by shrinking irrelevant coefficients while retaining influential ones, directly modeling inclusion uncertainty.\n\nCausal Inference Techniques: Explore advanced time-series methods if the primary goal is establishing causal links (use with caution).\nDomain Knowledge & Theory: Prioritize pre-selecting candidate factors and transformation ranges based on business logic, economic theory, and prior research. Validate final model components for plausibility and stability.\n\n\n\n\n\n\n\nImproper Use of Domain Knowledge\n\n\n\nExogenous factors in MMM are frequently confounded with media variables or other unobserved drivers. Consequently, the estimated coefficient for an exogenous variable may not represent its direct causal impact on the outcome but rather the statistical adjustment necessary to deconfound the estimated media effects. Rejecting such a variable because its coefficient sign contradicts simple causal expectations might inadvertently remove a necessary control variable, potentially leading to more biased estimates of media effectiveness.\n\n\nAdopting these more rigorous approaches is fundamental to developing media mix models that are statistically sound, reliable, and strategically valuable.",
    "crumbs": [
      "The Illusion of Significance"
    ]
  },
  {
    "objectID": "synthetic_data/simple_regression_data.html",
    "href": "synthetic_data/simple_regression_data.html",
    "title": "Simple Regression Data",
    "section": "",
    "text": "source\n\ngenerate_ols_data\n\n generate_ols_data (sample_size:int, n_exogenous_vars:int,\n                    n_confounder:int=0, noise_sigma:float=1.0,\n                    random_seed:Optional[int]=None)\n\nGenerate Simple OLS data\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsample_size\nint\n\n\n\n\nn_exogenous_vars\nint\n\nNumber of variables with a direct effect on the dep var\n\n\nn_confounder\nint\n0\nNumber of confounder variables to include\n\n\nnoise_sigma\nfloat\n1.0\nLevel of un-explained gaussian noise to add\n\n\nrandom_seed\nOptional\nNone\nRandom seed for reproducability\n\n\nReturns\nDataset\n\nGenerated Data\n\n\n\n\nSAMPLE_SIZE = 156\nN_INDEPVAR = 2\nN_CONFOUNDER = 2\nNOISE_SIGMA = 1\nRANDOM_SEED = 42\n\ndata = generate_ols_data(\n    SAMPLE_SIZE, N_INDEPVAR, \n    n_confounder=N_CONFOUNDER,\n    noise_sigma=NOISE_SIGMA, \n    random_seed=RANDOM_SEED)\ndata.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 240B\nDimensions:  (Index: 5)\nCoordinates:\n  * Index    (Index) int64 40B 0 1 2 3 4\nData variables:\n    var_0    (Index) float64 40B 0.5611 0.9553 -1.824 0.5083 0.162\n    var_1    (Index) float64 40B -1.173 0.6022 -1.697 -1.17 -1.124\n    con_0    (Index) float64 40B 1.744 0.828 0.06655 0.9896 0.7824\n    con_1    (Index) float64 40B 0.439 -0.2966 -0.6974 -1.178 -0.1907\n    depvar   (Index) float64 40B 2.91 -6.19 9.812 2.616 3.067\nAttributes:\n    true_betas:  {'var_0': -2.081, 'var_1': -4.826, 'con_0': 0.644, 'con_1': ...\n    true_alpha:  -1.216xarray.DatasetDimensions:Index: 5Coordinates: (1)Index(Index)int640 1 2 3 4array([0, 1, 2, 3, 4])Data variables: (5)var_0(Index)float640.5611 0.9553 -1.824 0.5083 0.162array([ 0.56108686,  0.9553213 , -1.82365978,  0.50834594,  0.16202801])var_1(Index)float64-1.173 0.6022 -1.697 -1.17 -1.124array([-1.17283017,  0.60223973, -1.69651489, -1.16964365, -1.12389828])con_0(Index)float641.744 0.828 0.06655 0.9896 0.7824array([1.74393453, 0.82798818, 0.06654582, 0.98958393, 0.78235034])con_1(Index)float640.439 -0.2966 ... -1.178 -0.1907array([ 0.43899316, -0.29657095, -0.69742382, -1.17830362, -0.19065106])depvar(Index)float642.91 -6.19 9.812 2.616 3.067array([ 2.91048549, -6.19019452,  9.81167712,  2.61633086,  3.06673432])Indexes: (1)IndexPandasIndexPandasIndex(Index([0, 1, 2, 3, 4], dtype='int64', name='Index'))Attributes: (2)true_betas :{'var_0': -2.081, 'var_1': -4.826, 'con_0': 0.644, 'con_1': 1.02}true_alpha :-1.216\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sythetic Data\n\n\n\n\n\n\n\n\n\nTable 1: OLS on synthetic data without controlling for confounds\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared:\n0.912\n\n\nModel:\nOLS\nAdj. R-squared:\n0.911\n\n\nMethod:\nLeast Squares\nF-statistic:\n795.4\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\n1.43e-81\n\n\nTime:\n18:17:04\nLog-Likelihood:\n-296.16\n\n\nNo. Observations:\n156\nAIC:\n598.3\n\n\nDf Residuals:\n153\nBIC:\n607.5\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-1.2774\n0.131\n-9.775\n0.000\n-1.536\n-1.019\n\n\nvar_0\n-2.2304\n0.134\n-16.666\n0.000\n-2.495\n-1.966\n\n\nvar_1\n-4.3309\n0.116\n-37.226\n0.000\n-4.561\n-4.101\n\n\n\n\n\n\n\n\nOmnibus:\n1.649\nDurbin-Watson:\n2.158\n\n\nProb(Omnibus):\n0.438\nJarque-Bera (JB):\n1.242\n\n\nSkew:\n-0.188\nProb(JB):\n0.537\n\n\nKurtosis:\n3.223\nCond. No.\n1.17\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: OLS fit on synthetic data without controlling for confounds\n\n\n\n\n\n\n\n\n\nTable 2: OLS on synthetic data controlling for confounds\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndepvar\nR-squared:\n0.965\n\n\nModel:\nOLS\nAdj. R-squared:\n0.965\n\n\nMethod:\nLeast Squares\nF-statistic:\n1055.\n\n\nDate:\nSat, 09 Nov 2024\nProb (F-statistic):\n3.26e-109\n\n\nTime:\n18:17:04\nLog-Likelihood:\n-223.44\n\n\nNo. Observations:\n156\nAIC:\n456.9\n\n\nDf Residuals:\n151\nBIC:\n472.1\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-1.2551\n0.083\n-15.145\n0.000\n-1.419\n-1.091\n\n\nvar_0\n-2.1015\n0.086\n-24.360\n0.000\n-2.272\n-1.931\n\n\nvar_1\n-4.8119\n0.093\n-51.774\n0.000\n-4.996\n-4.628\n\n\ncon_0\n0.6550\n0.086\n7.611\n0.000\n0.485\n0.825\n\n\ncon_1\n1.1080\n0.110\n10.041\n0.000\n0.890\n1.326\n\n\n\n\n\n\n\n\nOmnibus:\n1.168\nDurbin-Watson:\n1.685\n\n\nProb(Omnibus):\n0.558\nJarque-Bera (JB):\n1.216\n\n\nSkew:\n-0.132\nProb(JB):\n0.544\n\n\nKurtosis:\n2.657\nCond. No.\n2.21\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: OLS fit on synthetic data controlling for confounds",
    "crumbs": [
      "synthetic_data",
      "Simple Regression Data"
    ]
  },
  {
    "objectID": "utils/plotting.html",
    "href": "utils/plotting.html",
    "title": "Plotting Utils",
    "section": "",
    "text": "source\n\nrgb_to_hex\n\n rgb_to_hex (color:numpy.ndarray)\n\nConverts an RGB color array to a hex color string.\n\n\n\n\nType\nDetails\n\n\n\n\ncolor\nndarray\nNd array of color values\n\n\nReturns\nstr\nhex string of color data",
    "crumbs": [
      "utils",
      "Plotting Utils"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Common Regression Issues",
    "section": "",
    "text": "Please hop on over to the site, there I cover several topics that I’ve encountered while modelling. Please feel free to raise an issue if there is a topic that you think should be discussed, or if there are any changes/clarifications/errors you find on the site.",
    "crumbs": [
      "Common Regression Issues"
    ]
  },
  {
    "objectID": "index.html#sampling-errorsampling_error",
    "href": "index.html#sampling-errorsampling_error",
    "title": "Common Regression Issues",
    "section": "Sampling Error",
    "text": "Sampling Error\nThis section delves into the challenges posed by inaccuracies in independent variables within regression models. It highlights how such errors can lead to biased and inconsistent parameter estimates, even when sampling designs are unbiased and respondent answers are accurate. Through practical examples, like surveys assessing brand advertisement recall and its effect on sales, the section demonstrates the potential distortions sampling errors can introduce into regression outcomes. To address these issues, it explores strategies such as employing latent variable models and carefully considering the observation process to mitigate the negative impacts of sampling errors on regression analyses.",
    "crumbs": [
      "Common Regression Issues"
    ]
  },
  {
    "objectID": "index.html#multicollinearitymulticollinearity",
    "href": "index.html#multicollinearitymulticollinearity",
    "title": "Common Regression Issues",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nThis section examines the challenges posed by multicollinearity in regression models, particularly from a causal perspective. It utilizes Directed Acyclic Graphs (DAGs) to illustrate how multicollinearity can complicate the estimation of causal effects, using examples like assessing the impact of paid search impressions on sales. The section emphasizes the importance of identifying and adjusting for confounding variables to obtain unbiased estimates, highlighting that high Variance Inflation Factors (VIFs) can inflate standard errors and reduce estimate precision. It also discusses strategies to mitigate multicollinearity’s impact, such as focusing on relevant adjustment variables and refining causal models in collaboration with experts and stakeholders.",
    "crumbs": [
      "Common Regression Issues"
    ]
  },
  {
    "objectID": "index.html#the-illusion-of-significanceillusion",
    "href": "index.html#the-illusion-of-significanceillusion",
    "title": "Common Regression Issues",
    "section": "The Illusion of Significance",
    "text": "The Illusion of Significance\nStatistical models drive millions in spending decisions, yet beneath their precise-looking numbers lurks a dangerous problem. This post examines how the practice of selecting variables based on p-values creates a statistical house of cards; especially in Marketing Mix Modeling (MMM) and budget optimization. I show why common techniques like stepwise regression inevitably produce overconfident models with biased estimates that violate the very statistical principles they claim to uphold. These methodological flaws can result in money flowing to the wrong marketing channels based on illusory performance metrics. I demonstrate why Bayesian approaches offer a more honest alternative by naturally tempering overconfidence, incorporating what we already know, and providing intuitive uncertainty measures. Through techniques like spike-and-slab priors (or regularized horseshoe priors) and Bayesian Model Averaging (BMA), analysts can move beyond arbitrary significance thresholds toward probability-based decision-making. While Bayesian methods do require more computational horsepower and thoughtful prior specification, modern software has made them increasingly accessible. Using simulated examples inspired by real-world marketing and economic modeling, I show how Bayesian methods produce more reliable insights that lead to smarter budget allocation decisions.",
    "crumbs": [
      "Common Regression Issues"
    ]
  },
  {
    "objectID": "model_helpers/spline_models.html",
    "href": "model_helpers/spline_models.html",
    "title": "Splines",
    "section": "",
    "text": "source\n\nspline_component\n\n spline_component (knots:Union[int,List[~T]], index:Iterable[~T],\n                   degree:int=3)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nknots\nUnion\n\nNumber of knots or interior knots to use\n\n\nindex\nIterable\n\nindex\n\n\ndegree\nint\n3\nKnot degree defaults to cubic splines\n\n\n\n\nExample 1 (If knots are an integer)  \n\nN_SPLINES = 9\nINDEX = [i for i in range(156)]\nsplines = spline_component(\n    N_SPLINES,\n    INDEX\n)\n\n\n\n\n\n\n\n\n\nFigure 1: Representation of splines\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Samples of linear combination of splines.\n\n\n\n\n\n\n\nExample 2 (Knots can be directly specified)  \n\nINDEX = pd.date_range('01/01/2021', periods=156, freq=\"W-MON\")\nKNOTS = [INDEX[10], INDEX[30], INDEX[60], INDEX[120]]\nsplines_date = spline_component(\n    list(map(INDEX.get_loc, KNOTS)),\n    np.arange(len(INDEX))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3 (Spline Regression Model)  \n\nINDEX = np.linspace(0, 1, 156)\n\nspline_data = spline_component(19, INDEX, degree=3) # 19 Splines\nfun_ = lambda x: np.sin(2*np.pi*5*x)/(20*(x-.5)) + (x-.5) * 2\nfun = fun_(INDEX)# Complex Non-linear function to learn\n\ny_obs = fun + np.random.normal(0, .1, size=spline_data.shape[0]) # Noisy observation process\n\ntau_scale = 1.0\nc_scale = 0.1\nwith pm.Model() as model:\n    tau = pm.HalfCauchy('tau', tau_scale)\n    lambdas = pm.HalfCauchy(\"lambdas\", 1, shape=spline_data.shape[1])\n    c = pm.HalfCauchy(\"c\", beta=c_scale)\n\n    sigma_coeff = pm.Deterministic(\"sigma_coeff\", tau * lambdas / pm.math.sqrt(c**2 + (tau * lambdas)**2))\n    betas_ = pm.Normal(\"betas_\", 0, sigma_coeff)\n    betas = pm.Deterministic(\"betas\", pm.math.cumsum(betas_)) # Enforce Random Walk Process\n\n    alpha = pm.Normal(\"alpha\", 0, 1)\n\n    spline_trend = pm.Deterministic(\"splines\", spline_data@betas + alpha)\n    precision = pm.HalfCauchy(\"precision\", 2)\n    pm.Normal(\"mu\", mu=spline_trend, tau=precision, observed=y_obs)\n\n\n\n\n\n\n\n\n\nFigure 3: Graph of prior samples from the spline regression model.\n\n\n\n\n\n\n\n\n\nTable 1: Summary of coefficients for the spline regression model.\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbetas[0]\n0.009\n0.131\n-0.243\n0.262\n0.003\n0.002\n1407.0\n2094.0\n1.0\n\n\nbetas[1]\n-0.063\n0.118\n-0.281\n0.159\n0.003\n0.002\n2027.0\n2518.0\n1.0\n\n\nbetas[2]\n0.378\n0.114\n0.173\n0.598\n0.003\n0.002\n1621.0\n2392.0\n1.0\n\n\nbetas[3]\n0.548\n0.112\n0.324\n0.743\n0.003\n0.002\n1845.0\n2276.0\n1.0\n\n\nbetas[4]\n0.389\n0.109\n0.177\n0.590\n0.003\n0.002\n1625.0\n2538.0\n1.0\n\n\nbetas[5]\n0.197\n0.113\n-0.034\n0.391\n0.003\n0.002\n1607.0\n2097.0\n1.0\n\n\nbetas[6]\n0.977\n0.111\n0.757\n1.175\n0.003\n0.002\n1613.0\n1949.0\n1.0\n\n\nbetas[7]\n1.206\n0.113\n1.009\n1.435\n0.003\n0.002\n1537.0\n2096.0\n1.0\n\n\nbetas[8]\n0.692\n0.109\n0.469\n0.883\n0.002\n0.002\n1924.0\n2232.0\n1.0\n\n\nbetas[9]\n-0.551\n0.107\n-0.756\n-0.350\n0.003\n0.002\n1737.0\n2105.0\n1.0\n\n\nbetas[10]\n-0.544\n0.106\n-0.734\n-0.341\n0.002\n0.002\n2018.0\n2124.0\n1.0\n\n\nbetas[11]\n1.054\n0.110\n0.851\n1.265\n0.002\n0.002\n1973.0\n2353.0\n1.0\n\n\nbetas[12]\n1.833\n0.111\n1.622\n2.037\n0.003\n0.002\n1930.0\n2226.0\n1.0\n\n\nbetas[13]\n1.488\n0.111\n1.281\n1.702\n0.003\n0.002\n1957.0\n2474.0\n1.0\n\n\nbetas[14]\n1.369\n0.104\n1.173\n1.565\n0.003\n0.002\n1728.0\n2645.0\n1.0\n\n\nbetas[15]\n1.458\n0.109\n1.247\n1.655\n0.002\n0.002\n2259.0\n2227.0\n1.0\n\n\nbetas[16]\n2.077\n0.109\n1.865\n2.279\n0.003\n0.002\n1800.0\n1994.0\n1.0\n\n\nbetas[17]\n1.644\n0.133\n1.367\n1.864\n0.003\n0.002\n2406.0\n2565.0\n1.0\n\n\nbetas[18]\n1.939\n0.129\n1.687\n2.169\n0.003\n0.002\n2225.0\n2990.0\n1.0\n\n\nbetas[19]\n2.060\n0.106\n1.875\n2.274\n0.002\n0.001\n2802.0\n3035.0\n1.0\n\n\nalpha\n-1.054\n0.078\n-1.200\n-0.905\n0.002\n0.001\n2102.0\n2160.0\n1.0\n\n\ntau\n1.020\n5.563\n0.003\n2.581\n0.113\n0.080\n2561.0\n2382.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Posterior for horseshoe regularized spline regression.",
    "crumbs": [
      "model_helpers",
      "Splines"
    ]
  }
]